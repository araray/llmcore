{
  "model_id": "llama3.2:latest",
  "display_name": "Llama 3.2",
  "provider": "ollama",
  "model_type": "chat",
  "architecture": {
    "family": "llama",
    "parameter_count": "3B",
    "architecture_type": "transformer"
  },
  "context": {
    "max_input_tokens": 131072,
    "max_output_tokens": 8192,
    "default_output_tokens": 2048
  },
  "capabilities": {
    "streaming": true,
    "function_calling": true,
    "tool_use": true,
    "json_mode": true,
    "structured_output": true,
    "vision": false,
    "reasoning": false
  },
  "pricing": null,
  "lifecycle": {
    "release_date": "2024-09-25",
    "knowledge_cutoff": "2023-12",
    "status": "active"
  },
  "license": "llama3.2",
  "open_weights": true,
  "aliases": ["llama3.2", "llama3.2:3b"],
  "description": "Llama 3.2 is Meta's lightweight model optimized for edge deployment and local inference. Supports tool use and offers 128K context.",
  "tags": ["open-source", "local", "lightweight", "edge", "meta"],
  "provider_ollama": {
    "quantization_default": "q4_K_M",
    "available_quantizations": ["q4_K_M", "q5_K_M", "q8_0", "fp16"],
    "vram_requirement_gb": 3
  },
  "source": "builtin"
}
