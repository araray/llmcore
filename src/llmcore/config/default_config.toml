# ==============================================================================
# LLMCore Default Configuration
# ==============================================================================
# This is the default configuration file for the LLMCore library.
# It defines sensible defaults for all configurable parameters.
# Users can override these settings via:
#   1. User config file: ~/.config/llmcore/config.toml
#   2. Custom config file: Specified via LLMCore.create(config_file_path=...)
#   3. Environment variables: Prefixed with LLMCORE_ (e.g., LLMCORE_DEFAULT_PROVIDER)
#   4. Direct overrides: Passed via LLMCore.create(config_overrides=...)

# ==============================================================================
# [llmcore] - Core Library Settings
# ==============================================================================
[llmcore]

# --- Default LLM Provider ---
# What it does: Specifies which provider instance (from [providers] below) to use
#               by default when no provider is explicitly chosen.
# Purpose: Allows you to switch between different LLM backends quickly.
# Possible Values: The name of any configured provider instance (e.g., "ollama",
#                  "openai", "anthropic", "gemini").
# Default Value: "ollama"
# Environment Variable: LLMCORE_DEFAULT_PROVIDER
default_provider = "ollama"

# --- Default Embedding Model ---
# What it does: Specifies which embedding model to use for RAG operations.
# Purpose: Required for generating vector embeddings of text chunks in RAG.
# Possible Values:
#   - For local models (Sentence Transformers): A model name from the Hugging Face
#     model hub (e.g., "all-MiniLM-L6-v2", "all-mpnet-base-v2").
#     The model will be downloaded automatically.
#   - For service-based models: A string in the format "provider:model_name"
#     (e.g., "openai:text-embedding-3-small", "google:models/embedding-001").
# Default Value: "all-MiniLM-L6-v2"
# Environment Variable: LLMCORE_DEFAULT_EMBEDDING_MODEL
default_embedding_model = "all-MiniLM-L6-v2"

# --- Logging Verbosity ---
# What it does: Configures the logging verbosity for the entire `llmcore` library.
# Purpose: Control the amount of diagnostic information emitted by the library.
# Possible Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
# Default Value: "INFO"
# Environment Variable: LLMCORE_LOG_LEVEL
log_level = "INFO"

# --- Raw Payload Logging (for Debugging) ---
# What it does: If true, `llmcore` will log the exact raw JSON request and
#               response payloads exchanged with the underlying LLM provider APIs.
#               This logging occurs at the `DEBUG` log level.
# Purpose: An indispensable tool for advanced troubleshooting. It allows you to
#          inspect the precise data being sent and received, which is essential
#          for diagnosing provider errors or understanding unexpected behavior.
# Possible Options: true, false
# Default Value: false
# Environment Variable: LLMCORE_LOG_RAW_PAYLOADS
log_raw_payloads = false

# --- Administrative API Key ---
# What it does: Defines the secret key required for accessing administrative endpoints
#               such as live configuration reloading. This provides a separate,
#               high-privilege authentication layer independent of tenant API keys.
# Purpose: Secures administrative operations that could impact the entire platform,
#          ensuring only authorized administrators can perform sensitive actions
#          like configuration reloads.
# Security: This key should be treated as a high-privilege secret. In production,
#           ALWAYS set this via the environment variable rather than in config files.
# Default Value: "" (empty - admin endpoints will be disabled)
# Environment Variable: LLMCORE_ADMIN_API_KEY
# Example: admin_api_key = "admin_llmk_abc123def456ghi789jkl"
admin_api_key = ""

# ==============================================================================
# [providers] - LLM Provider Configurations
# ==============================================================================
# This section is where all individual LLM provider instances are defined.
# You can define multiple instances of the same provider type by giving them
# unique names (e.g., [providers.my_openai_clone]) and setting `type = "openai"`.
[providers]

  # --- OpenAI Provider ---
  # For official OpenAI models and any OpenAI-compatible API (e.g., Groq, Anyscale).
  [providers.openai]
  # API Key: It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_PROVIDERS__OPENAI__API_KEY or OPENAI_API_KEY
  # api_key = "sk-..."

  # Base URL: Override for proxies or compatible APIs.
  # Example for Groq: base_url = "https://api.groq.com/openai/v1"
  # base_url = "https://api.openai.com/v1"

  # Default model for this provider instance.
  default_model = "gpt-4o"

  # Request timeout in seconds.
  timeout = 60

  # --- Anthropic Provider ---
  # For Claude models.
  [providers.anthropic]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__ANTHROPIC__API_KEY or ANTHROPIC_API_KEY
  # api_key = "sk-ant-..."

  # Default model for this provider instance.
  default_model = "claude-3-opus-20240229"
  timeout = 60

  # --- Ollama Provider ---
  # For locally running models via Ollama.
  [providers.ollama]
  # Host URL of the running Ollama server.
  # The official `ollama` Python library uses this parameter.
  # host = "http://localhost:11434"

  # Default local model to use. Ensure you have run `ollama pull <model_name>`.
  default_model = "llama3"

  # Longer timeout is recommended for local models.
  timeout = 120

  # Tokenizer for token counting. `tiktoken_cl100k_base` is a good default.
  # Use 'char_div_4' as a rough fallback if tiktoken causes issues with a model.
  # tokenizer = "tiktoken_cl100k_base"

  # --- Google Gemini Provider ---
  # For Gemini models.
  [providers.gemini]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__GEMINI__API_KEY or GOOGLE_API_KEY
  # api_key = "..."

  # Default model for this provider instance.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60

  # Optional: Configure Google AI's safety filters.
  # This table is passed directly to the `google-genai` SDK.
  # [providers.gemini.safety_settings]
  # HARM_CATEGORY_HARASSMENT = "BLOCK_NONE"
  # HARM_CATEGORY_HATE_SPEECH = "BLOCK_NONE"


# ==============================================================================
# [storage] - Persistence Configurations
# ==============================================================================
# This section governs where and how persistent data, such as conversation
# history (session memory) and RAG documents (semantic memory), is stored.
[storage]

  # --- Session & Episodic Memory Storage ---
  # Configures the backend for storing conversation history and agent episodes.
  [storage.session]
  # Type: Determines the storage backend.
  #   - "json": Simple, human-readable. Good for low-concurrency apps.
  #   - "sqlite": (Default) Excellent for single-process apps and local dev.
  #   - "postgres": Recommended for production, especially with multiple workers.
  # Environment Variable: LLMCORE_STORAGE__SESSION__TYPE
  type = "sqlite"

  # Path: Filesystem path for "json" and "sqlite" backends.
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_STORAGE__SESSION__PATH
  path = "~/.llmcore/sessions.db"

  # Database URL: Full connection URL for "postgres".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__SESSION__DB_URL
  # Example: db_url = "postgresql://user:password@host:port/database_name"
  db_url = ""

  # --- Vector Storage for RAG (Semantic Memory) ---
  # Configures the vector store for RAG documents and their embeddings.
  [storage.vector]
  # Type: Determines the vector database backend.
  #   - "chromadb": (Default) Easy to set up for local development.
  #   - "pgvector": Excellent for production if you already use PostgreSQL.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__TYPE
  type = "chromadb"

  # Default Collection: A namespace for RAG documents. Allows you to maintain
  # multiple distinct knowledge bases in the same vector store.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DEFAULT_COLLECTION
  default_collection = "llmcore_default_rag"

  # Path: Filesystem path for file-based vector stores like "chromadb".
  # Environment Variable: LLMCORE_STORAGE__VECTOR__PATH
  path = "~/.llmcore/chroma_db"

  # Database URL: Connection URL for "pgvector".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DB_URL
  db_url = ""


# ==============================================================================
# [embedding] - Vectorization Model Configurations
# ==============================================================================
# This section configures the models responsible for converting text into
# numerical vectors (embeddings) for the RAG system. The global
# `llmcore.default_embedding_model` setting determines which model to use.
# If that identifier contains a colon (e.g., "openai:text-embedding-3-small"),
# the part before the colon tells the system which configuration block below
# to consult for specific settings like API keys.
[embedding]

  [embedding.openai]
  # API Key for OpenAI embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__OPENAI__API_KEY
  # api_key = ""
  default_model = "text-embedding-3-small"
  # base_url = "..." # Optional proxy URL

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__GOOGLE__API_KEY
  # api_key = ""
  default_model = "models/embedding-001"

  [embedding.ollama]
  # Host URL for the Ollama server for embeddings.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings. Ensure it's pulled locally.
  # Examples: "mxbai-embed-large", "nomic-embed-text"
  default_model = "mxbai-embed-large"
  timeout = 60

  [embedding.sentence_transformer]
  # Additional configuration for local `sentence-transformers` models.
  # The model itself is specified in `llmcore.default_embedding_model`.
  # Device to run the model on. If not set, the library will auto-detect.
  # Options: "cpu", "cuda", "mps"
  # device = "cpu"


# ==============================================================================
# [context_management] - Advanced Prompt Control
# ==============================================================================
# This section is the control center for prompt engineering. It governs the
# sophisticated, multi-stage process of assembling the final context that is
# sent to the LLM. The system first prioritizes what to INCLUDE, and only then
# decides what to TRUNCATE if the model's token limit is exceeded.
[context_management]

  # --- Inclusion Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               components are added to the prompt.
  # Purpose: Gives fine-grained control over the final prompt's structure.
  # Valid Components: "system_history", "explicitly_staged", "user_items_active",
  #                   "history_chat", "final_user_query".
  # Default: "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"
  inclusion_priority = "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"

  # --- Truncation Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               types are removed to meet token limits.
  # Purpose: Controls what information is sacrificed when the context is too long.
  # Valid Components: "history_chat", "user_items_active", "rag_in_query",
  #                   "explicitly_staged".
  # Default: "history_chat,user_items_active,rag_in_query,explicitly_staged"
  truncation_priority = "history_chat,user_items_active,rag_in_query,explicitly_staged"

  # --- RAG Prompt Template ---
  # What it does: The template used to format the final query when RAG is enabled.
  #               It will be formatted with `{context}` (retrieved documents) and
  #               `{question}` (the user's query).
  # Purpose: To instruct the LLM on how to use the provided RAG context.
  default_prompt_template = """You are an AI assistant specialized in answering questions about codebases
  based on provided context.
  Use ONLY the following pieces of retrieved context to answer the user's question.
  If the answer is not found in the context, state that you cannot answer based on the provided information.
  Do not make up an answer or use external knowledge. Keep the answer concise and relevant to the question.
  Include relevant source file paths and line numbers if possible, based *only* on the provided context metadata.

  Context:
  ---------------------
  {context}
  ---------------------

  Question: {question}

  Answer:"""

    # Optional path to a custom prompt template file. Overrides the string above.
    prompt_template_path = ""

    # Default number of documents to retrieve for RAG.
    rag_retrieval_k = 3

    # Number of tokens to leave free in the context window for the model's response.
    reserved_response_tokens = 500

    # Minimum number of recent chat messages to try to keep during truncation.
    minimum_history_messages = 1

    # Prioritizes keeping the N most recent user messages (and their preceding
    # assistant responses) during truncation.
    user_retained_messages_count = 5

    # A safeguard character limit for a single user-provided context item.
    # The default is very large to accommodate large documents.
    max_chars_per_user_item = 40000000


    # ==============================================================================
    # [semantiscan] - Semantiscan RAG Engine Configuration (Phase 1 Skeleton)
    # ==============================================================================
    # This section configures the semantiscan library when used for code ingestion
    # and RAG operations. In Phase 2, semantiscan will be refactored to read its
    # configuration from this section instead of maintaining separate config files.
    #
    # Phase 1 Status: SKELETON ONLY - Example placeholders for planning purposes.
    # Phase 2 Will: Implement full Pydantic models in semantiscan to parse this section.

    [semantiscan]

    # --- Database / Vector Store Configuration ---
    # Defines where semantiscan stores embeddings and searchable metadata.
    # Phase 2 Note: Maps to semantiscan.core.storage.chromadb_client.ChromaDBClient
    database_type = "chromadb"  # Currently only "chromadb" is supported
    database_path = "~/.local/share/semantiscan/semantiscan_db"  # Persistent vector store path
    default_collection = "codebase"  # Default ChromaDB collection name

    # --- External Metadata Store Configuration ---
    # Optional SQLite database for rich metadata (Git info, commit messages, etc.)
    # that doesn't fit well in the vector store.
    # Phase 2 Note: Maps to semantiscan.core.storage.metadata_store_client.MetadataStoreClient
    metadata_store_enabled = false  # Set to true to enable external metadata storage
    metadata_store_path = "~/.local/share/semantiscan/metadata.sqlite"  # SQLite DB path
    metadata_store_table = "chunk_metadata"  # Table for chunk-level metadata
    metadata_ingestion_log_table = "ingestion_log"  # Tracks last ingested commit per repo
    metadata_file_history_table = "file_history"  # Tracks file changes per commit

    # --- Chunking Configuration ---
    # Controls how source files are split into indexable chunks.
    # Phase 2 Note: Maps to semantiscan.core.chunking.engine.ChunkingEngine
    chunking_max_chunk_tokens = 512  # Maximum tokens per chunk (for embedding limits)
    chunking_default_strategy = "agnostic"  # "antlr" | "format" | "agnostic"

    # --- Embedding Configuration ---
    # Specifies which embedding model to use for semantiscan's ingestion pipeline.
    # Phase 2 Note: semantiscan will call llmcore's EmbeddingManager via the
    # llmcore instance passed to it, but this setting defines the default model choice.
    embedding_model = "all-MiniLM-L6-v2"  # Default: local Sentence Transformer model

    # --- LLM Configuration for Query Synthesis ---
    # When semantiscan generates answers to queries, it uses llmcore for the LLM call.
    # These settings define which provider/model to use by default.
    # Phase 2 Note: semantiscan.pipelines.query.QueryPipeline will use these values
    # when calling llmcore.chat()
    llm_provider = "ollama"  # Must match a provider name in [providers] section
    llm_model = "mistral:latest"  # Model name for the specified provider
    llm_temperature = 0.7  # Temperature for answer generation
    llm_max_tokens = 2000  # Max tokens in generated answers

    # --- Retrieval Configuration ---
    # Controls how semantiscan retrieves relevant chunks for RAG queries.
    # Phase 2 Note: Maps to semantiscan.core.retrieval.retriever.Retriever
    retrieval_top_k = 7  # Number of most relevant chunks to retrieve
    retrieval_enable_hybrid = true  # Use hybrid search (semantic + keyword BM25)
    retrieval_enrich_with_metadata = false  # Include external metadata in context

    # --- File Discovery Configuration ---
    # Controls which files are included/excluded during ingestion.
    # Phase 2 Note: Maps to semantiscan.pipelines.ingest.IngestionPipeline
    discovery_use_gitignore = true  # Respect .gitignore rules during scanning
    discovery_excluded_dirs = [  # Directories to always skip
        "__pycache__",
        "node_modules",
        ".git",
        "venv",
        ".venv",
        "build",
        "dist"
    ]
    discovery_excluded_files = [".DS_Store"]  # Files to always skip

    # --- Git Integration (for historical/incremental ingestion modes) ---
    # Phase 2 Note: For future git-aware ingestion features
    git_default_ref = "main"  # Default branch/ref if not specified
    git_enable_commit_analysis = false  # Extract commit messages, authors, etc.

    # --- Logging Configuration ---
    # Semantiscan-specific logging settings (separate from llmcore's log_level).
    # Phase 2 Note: semantiscan will configure its own logger using these settings
    log_level = "INFO"  # "DEBUG" | "INFO" | "WARNING" | "ERROR" | "CRITICAL"
    log_to_file = false  # Enable file logging for semantiscan operations
    log_file_path = "~/.local/share/semantiscan/logs/semantiscan.log"
