# ==============================================================================
# LLMCore Default Configuration
# ==============================================================================
# This is the default configuration file for the LLMCore library.
# It defines sensible defaults for all configurable parameters.
# Users can override these settings via:
#   1. User config file: ~/.config/llmcore/config.toml
#   2. Custom config file: Specified via LLMCore.create(config_file_path=...)
#   3. Environment variables: Prefixed with LLMCORE_ (e.g., LLMCORE_DEFAULT_PROVIDER)
#   4. Direct overrides: Passed via LLMCore.create(config_overrides=...)

# ==============================================================================
# [llmcore] - Core Library Settings
# ==============================================================================
[llmcore]

# --- Default LLM Provider ---
# What it does: Specifies which provider instance (from [providers] below) to use
#               by default when no provider is explicitly chosen.
# Purpose: Allows you to switch between different LLM backends quickly.
# Possible Values: The name of any configured provider instance (e.g., "ollama",
#                  "openai", "anthropic", "gemini").
# Default Value: "ollama"
# Environment Variable: LLMCORE_DEFAULT_PROVIDER
default_provider = "ollama"

# --- Default Embedding Model ---
# What it does: Specifies which embedding model to use for RAG operations.
# Purpose: Required for generating vector embeddings of text chunks in RAG.
# Possible Values:
#   - For local models (Sentence Transformers): A model name from the Hugging Face
#     model hub (e.g., "all-MiniLM-L6-v2", "all-mpnet-base-v2").
#     The model will be downloaded automatically.
#   - For service-based models: A string in the format "provider:model_name"
#     (e.g., "openai:text-embedding-3-small", "google:models/embedding-001").
# Default Value: "all-MiniLM-L6-v2"
# Environment Variable: LLMCORE_DEFAULT_EMBEDDING_MODEL
default_embedding_model = "all-MiniLM-L6-v2"

# --- Logging Verbosity ---
# What it does: Configures the logging verbosity for the entire `llmcore` library.
# Purpose: Control the amount of diagnostic information emitted by the library.
# Possible Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
# Default Value: "INFO"
# Environment Variable: LLMCORE_LOG_LEVEL
log_level = "INFO"

# --- Raw Payload Logging (for Debugging) ---
# What it does: If true, `llmcore` will log the exact raw JSON request and
#               response payloads exchanged with the underlying LLM provider APIs.
#               This logging occurs at the `DEBUG` log level.
# Purpose: An indispensable tool for advanced troubleshooting. It allows you to
#          inspect the precise data being sent and received, which is essential
#          for diagnosing provider errors or understanding unexpected behavior.
# Possible Options: true, false
# Default Value: false
# Environment Variable: LLMCORE_LOG_RAW_PAYLOADS
log_raw_payloads = false

# --- Administrative API Key ---
# What it does: Defines the secret key required for accessing administrative endpoints
#               such as live configuration reloading. This provides a separate,
#               high-privilege authentication layer independent of tenant API keys.
# Purpose: Secures administrative operations that could impact the entire platform,
#          ensuring only authorized administrators can perform sensitive actions
#          like configuration reloads.
# Security: This key should be treated as a high-privilege secret. In production,
#           ALWAYS set this via the environment variable rather than in config files.
# Default Value: "" (empty - admin endpoints will be disabled)
# Environment Variable: LLMCORE_ADMIN_API_KEY
# Example: admin_api_key = "admin_llmk_abc123def456ghi789jkl"
admin_api_key = ""

# =============================================================================
# UNIFIED LOGGING CONFIGURATION
# =============================================================================
# This section controls logging for ALL components: llmcore, llmchat,
# semantiscan, and confy. Configuration here is the single source of truth.

[logging]
# Console logging - set to false to suppress ALL console output
console_enabled = false
console_level = "WARNING"        # Minimum level for console (if enabled)
console_format = "%(levelname)s - %(message)s"

# File logging - always recommended to keep enabled
file_enabled = true
file_level = "DEBUG"             # Capture full detail in files
file_directory = "~/.local/share/llmcore/logs"
file_name_pattern = "{app}_{timestamp:%Y%m%d_%H%M%S}.log"
file_format = "%(asctime)s [%(levelname)-8s] %(name)-30s - %(message)s (%(filename)s:%(lineno)d)"

# Per-component log levels (override the handlers' levels for specific loggers)
[logging.components]
llmchat = "INFO"
llmcore = "INFO"
semantiscan = "INFO"
confy = "WARNING"
# Suppress noisy third-party libraries
urllib3 = "WARNING"
httpx = "WARNING"
httpcore = "WARNING"
asyncio = "WARNING"
aiosqlite = "WARNING"
chromadb = "WARNING"
posthog = "ERROR"

# ==============================================================================
# [providers] - LLM Provider Configurations
# ==============================================================================
# This section is where all individual LLM provider instances are defined.
# You can define multiple instances of the same provider type by giving them
# unique names (e.g., [providers.my_openai_clone]) and setting `type = "openai"`.
[providers]

  # --- OpenAI Provider ---
  # For official OpenAI models and any OpenAI-compatible API (e.g., Groq, Anyscale).
  [providers.openai]
  # API Key: It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_PROVIDERS__OPENAI__API_KEY or OPENAI_API_KEY
  # api_key = "sk-..."

  # Base URL: Override for proxies or compatible APIs.
  # Example for Groq: base_url = "https://api.groq.com/openai/v1"
  # base_url = "https://api.openai.com/v1"

  # Default model for this provider instance.
  default_model = "gpt-4o"

  # Request timeout in seconds.
  timeout = 60

  # --- Anthropic Provider ---
  # For Claude models.
  [providers.anthropic]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__ANTHROPIC__API_KEY or ANTHROPIC_API_KEY
  # api_key = "sk-ant-..."

  # Default model for this provider instance.
  default_model = "claude-3-opus-20240229"
  timeout = 60

  # --- Ollama Provider ---
  # For locally running models via Ollama.
  [providers.ollama]
  # Host URL of the running Ollama server.
  # The official `ollama` Python library uses this parameter.
  # host = "http://localhost:11434"

  # Default local model to use. Ensure you have run `ollama pull <model_name>`.
  default_model = "llama3"

  # Longer timeout is recommended for local models.
  timeout = 120

  # Tokenizer for token counting. `tiktoken_cl100k_base` is a good default.
  # Use 'char_div_4' as a rough fallback if tiktoken causes issues with a model.
  # tokenizer = "tiktoken_cl100k_base"

  # --- Google Gemini Provider ---
  # For Gemini models.
  [providers.gemini]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__GEMINI__API_KEY or GOOGLE_API_KEY
  # api_key = "..."

  # Default model for this provider instance.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60

  # Optional: Configure Google AI's safety filters.
  # This table is passed directly to the `google-genai` SDK.
  # [providers.gemini.safety_settings]
  # HARM_CATEGORY_HARASSMENT = "BLOCK_NONE"
  # HARM_CATEGORY_HATE_SPEECH = "BLOCK_NONE"


# ==============================================================================
# [storage] - Persistence Configurations
# ==============================================================================
# This section governs where and how persistent data, such as conversation
# history (session memory) and RAG documents (semantic memory), is stored.
[storage]

  # --- Session & Episodic Memory Storage ---
  # Configures the backend for storing conversation history and agent episodes.
  [storage.session]
  # Type: Determines the storage backend.
  #   - "json": Simple, human-readable. Good for low-concurrency apps.
  #   - "sqlite": (Default) Excellent for single-process apps and local dev.
  #   - "postgres": Recommended for production, especially with multiple workers.
  # Environment Variable: LLMCORE_STORAGE__SESSION__TYPE
  type = "sqlite"

  # Path: Filesystem path for "json" and "sqlite" backends.
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_STORAGE__SESSION__PATH
  path = "~/.llmcore/sessions.db"

  # Database URL: Full connection URL for "postgres".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__SESSION__DB_URL
  # Example: db_url = "postgresql://user:password@host:port/database_name"
  db_url = ""

  # --- Vector Storage for RAG (Semantic Memory) ---
  # Configures the vector store for RAG documents and their embeddings.
  [storage.vector]
  # Type: Determines the vector database backend.
  #   - "chromadb": (Default) Easy to set up for local development.
  #   - "pgvector": Excellent for production if you already use PostgreSQL.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__TYPE
  type = "chromadb"

  # Default Collection: A namespace for RAG documents. Allows you to maintain
  # multiple distinct knowledge bases in the same vector store.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DEFAULT_COLLECTION
  default_collection = "llmcore_default_rag"

  # Path: Filesystem path for file-based vector stores like "chromadb".
  # Environment Variable: LLMCORE_STORAGE__VECTOR__PATH
  path = "~/.llmcore/chroma_db"

  # Database URL: Connection URL for "pgvector".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DB_URL
  db_url = ""

  # ==============================================================================
  # [model_cards] - Model Card Library Configuration
  # ==============================================================================
  # The Model Card Library provides comprehensive metadata for LLM models,
  # including context limits, capabilities, pricing, and lifecycle information.
  # Cards are loaded from built-in defaults and can be overridden by user cards.
  [model_cards]

  # Path to user-defined model cards directory.
  # Cards in this directory override built-in cards with the same model_id.
  # Organize cards by provider subdirectory: model_cards/<provider>/<model>.json
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_MODEL_CARDS__USER_CARDS_PATH
  user_cards_path = "~/.config/llmcore/model_cards"

  # Automatically load model cards on first registry access.
  # Set to false if you want to control loading timing manually via registry.load()
  # Environment Variable: LLMCORE_MODEL_CARDS__AUTO_LOAD
  auto_load = true

  # Validation strictness for model card files.
  # If true, invalid JSON files will raise errors during loading.
  # If false, invalid files are logged as warnings and skipped.
  # Environment Variable: LLMCORE_MODEL_CARDS__STRICT_VALIDATION
  strict_validation = false

# ==============================================================================
# [embedding] - Vectorization Model Configurations
# ==============================================================================
# This section configures the models responsible for converting text into
# numerical vectors (embeddings) for the RAG system. The global
# `llmcore.default_embedding_model` setting determines which model to use.
# If that identifier contains a colon (e.g., "openai:text-embedding-3-small"),
# the part before the colon tells the system which configuration block below
# to consult for specific settings like API keys.
[embedding]

  [embedding.openai]
  # API Key for OpenAI embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__OPENAI__API_KEY
  # api_key = ""
  default_model = "text-embedding-3-small"
  # base_url = "..." # Optional proxy URL

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__GOOGLE__API_KEY
  # api_key = ""
  default_model = "models/embedding-001"

  [embedding.ollama]
  # Host URL for the Ollama server for embeddings.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings. Ensure it's pulled locally.
  # Examples: "mxbai-embed-large", "nomic-embed-text"
  default_model = "mxbai-embed-large"
  timeout = 60

  [embedding.sentence_transformer]
  # Additional configuration for local `sentence-transformers` models.
  # The model itself is specified in `llmcore.default_embedding_model`.
  # Device to run the model on. If not set, the library will auto-detect.
  # Options: "cpu", "cuda", "mps"
  # device = "cpu"


# ==============================================================================
# [context_management] - Advanced Prompt Control
# ==============================================================================
# This section is the control center for prompt engineering. It governs the
# sophisticated, multi-stage process of assembling the final context that is
# sent to the LLM. The system first prioritizes what to INCLUDE, and only then
# decides what to TRUNCATE if the model's token limit is exceeded.
[context_management]

  # --- Inclusion Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               components are added to the prompt.
  # Purpose: Gives fine-grained control over the final prompt's structure.
  # Valid Components: "system_history", "explicitly_staged", "user_items_active",
  #                   "history_chat", "final_user_query".
  # Default: "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"
  inclusion_priority = "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"

  # --- Truncation Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               types are removed to meet token limits.
  # Purpose: Controls what information is sacrificed when the context is too long.
  # Valid Components: "history_chat", "user_items_active", "rag_in_query",
  #                   "explicitly_staged".
  # Default: "history_chat,user_items_active,rag_in_query,explicitly_staged"
  truncation_priority = "history_chat,user_items_active,rag_in_query,explicitly_staged"

  # --- RAG Prompt Template ---
  # What it does: The template used to format the final query when RAG is enabled.
  #               It will be formatted with `{context}` (retrieved documents) and
  #               `{question}` (the user's query).
  # Purpose: To instruct the LLM on how to use the provided RAG context.
  default_prompt_template = """You are an AI assistant specialized in answering questions about codebases
  based on provided context.
  Use ONLY the following pieces of retrieved context to answer the user's question.
  If the answer is not found in the context, state that you cannot answer based on the provided information.
  Do not make up an answer or use external knowledge. Keep the answer concise and relevant to the question.
  Include relevant source file paths and line numbers if possible, based *only* on the provided context metadata.

  Context:
  ---------------------
  {context}
  ---------------------

  Question: {question}

  Answer:"""

    # Optional path to a custom prompt template file. Overrides the string above.
    prompt_template_path = ""

    # Default number of documents to retrieve for RAG.
    rag_retrieval_k = 3

    # Number of tokens to leave free in the context window for the model's response.
    reserved_response_tokens = 500

    # Minimum number of recent chat messages to try to keep during truncation.
    minimum_history_messages = 1

    # Prioritizes keeping the N most recent user messages (and their preceding
    # assistant responses) during truncation.
    user_retained_messages_count = 5

    # A safeguard character limit for a single user-provided context item.
    # The default is very large to accommodate large documents.
    max_chars_per_user_item = 40000000

    # ==============================================================================
    # Step 2.1: Semantiscan Configuration Section (CORRECTED & COMPLETE)
    # ==============================================================================
    # Purpose: Unified configuration for the semantiscan RAG/ingestion engine.
    # After Phase 2, semantiscan no longer maintains its own config.toml;
    # instead, it receives configuration from llmcore via this section.
    #
    # References:
    # - semantiscan/config/loaders.py (DEFAULT_CONFIG)
    # - semantiscan/config/models.py (AppConfig schema)
    # - Resurrection Plan Section VI, Task 1.3 & Phase 2, Step 2.1
    # ==============================================================================

    [semantiscan]
    # Top-level semantiscan enablement flag (for future use)
    enabled = true

        # --- Database Configuration (Vector Store) ---
        # Defines where semantiscan stores embedded code chunks for RAG retrieval.
        # IMPORTANT: For unified storage, this should match llmcore's [storage.vector.path]
        [semantiscan.database]
        type = "chromadb"  # Currently only ChromaDB is supported by semantiscan
        # Default path - should be synchronized with llmcore's vector storage path
        path = "~/.llmcore/chroma_db"
        collection_name = "default_semantiscan"  # Default collection for ingested code

        # --- Metadata Store Configuration ---
        # Optional SQLite database for tracking ingestion history, Git metadata, etc.
        # CORRECTED: Field names now match semantiscan.config.models.MetadataStoreConfig
        [semantiscan.metadata_store]
        enable = false  # Disabled by default; enable for Git-aware ingestion tracking
        type = "sqlite"  # Type of external store (sqlite or postgresql - postgresql future)
        path = "~/.local/share/semantiscan/metadata.db"
        connection_string = ""  # Connection string for PostgreSQL (required if type is postgresql)
        # Primary table for storing rich chunk metadata
        table_name = "chunk_metadata"
        # Table for tracking ingestion state per repo/branch
        ingestion_log_table_name = "ingestion_log"
        # Table for tracking file changes per commit ('historical_delta' mode)
        file_history_table_name = "file_history"

        # --- Embedding Configuration ---
        # Defines which embedding models are available for chunking ingestion.
        # semantiscan will use llmcore's EmbeddingManager for actual embedding generation.
        [semantiscan.embeddings]
        default_model = "sentence_transformer_local"  # Default embedding model key

        # Define available embedding models (keys reference llmcore's [embedding.*] sections)
        [semantiscan.embeddings.models.sentence_transformer_local]
        provider = "sentence-transformers"  # Maps to llmcore's sentence_transformer provider
        model_name = "all-MiniLM-L6-v2"  # Default local model
        device = "cpu"  # "cpu" | "cuda" | "mps" (for M1/M2 Macs)
        max_request_tokens = 8000  # Max tokens per embedding batch request
        base_url = ""  # Not used for sentence-transformers
        tokenizer_name = ""  # Optional: Specific tokenizer name
        uses_doc_query_prefixes = false  # Set to true if model requires different prefixes
        query_prefix = ""  # Prefix for queries if uses_doc_query_prefixes is true
        document_prefix = ""  # Prefix for documents if uses_doc_query_prefixes is true

        [semantiscan.embeddings.models.openai_ada]
        provider = "openai"  # Maps to llmcore's openai embedding provider
        model_name = "text-embedding-ada-002"
        device = "cpu"  # Not used for API-based models
        api_key_env = "OPENAI_API_KEY"  # Reference to environment variable
        max_request_tokens = 8000
        base_url = ""  # Optional: Custom API endpoint
        tokenizer_name = "cl100k_base"  # OpenAI tokenizer
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        [semantiscan.embeddings.models.openai_large]
        provider = "openai"
        model_name = "text-embedding-3-large"
        device = "cpu"
        api_key_env = "OPENAI_API_KEY"
        max_request_tokens = 8000
        base_url = ""
        tokenizer_name = "cl100k_base"
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        [semantiscan.embeddings.models.ollama_nomic]
        provider = "ollama"
        model_name = "nomic-embed-text:latest"
        device = "cpu"  # Not used for Ollama
        api_key_env = ""  # Not needed for local Ollama
        max_request_tokens = 2048
        base_url = "http://localhost:11434"
        tokenizer_name = ""  # Optional
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        # --- Chunking Configuration ---
        # Defines how different file types are parsed and chunked during ingestion.
        [semantiscan.chunking]
        default_strategy = "RecursiveSplitter"  # Fallback for unknown file types

        # ANTLR-based code parsers (requires ANTLR runtime and generated parsers)
        [semantiscan.chunking.strategies.python_code]
        extensions = [".py", ".pyw"]
        grammar = "Python3"
        entry_points = ["funcdef", "async_funcdef", "classdef", "decorated"]
        parser = ""  # Not used for ANTLR strategy
        method = ""  # Not used for ANTLR strategy
        strategy_sequence = []  # Not used for simple strategy
        hybrid_content = false  # Combine related elements in ANTLR chunks

        [semantiscan.chunking.strategies.java_code]
        extensions = [".java"]
        grammar = "Java"
        entry_points = ["methodDeclaration", "classDeclaration", "interfaceDeclaration"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.cpp_code]
        extensions = [".cpp", ".cc", ".cxx", ".hpp", ".h", ".hxx"]
        grammar = "CPP14"
        entry_points = ["functionDefinition", "classSpecifier"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.go_code]
        extensions = [".go"]
        grammar = "Golang"
        entry_points = ["functionDecl", "methodDecl", "typeSpec"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        # Format-specific parsers (YAML, JSON, TOML, XML, etc.)
        [semantiscan.chunking.strategies.yaml_format]
        extensions = [".yaml", ".yml"]
        grammar = ""  # Not used for format strategy
        entry_points = []
        parser = "PyYAML"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.json_format]
        extensions = [".json"]
        grammar = ""
        entry_points = []
        parser = "json"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.toml_format]
        extensions = [".toml"]
        grammar = ""
        entry_points = []
        parser = "tomli"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.xml_format]
        extensions = [".xml"]
        grammar = ""
        entry_points = []
        parser = "xml.etree.ElementTree"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        # Agnostic text splitters (for markdown, plain text, logs, etc.)
        [semantiscan.chunking.strategies.markdown_agnostic]
        extensions = [".md", ".markdown"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.text_agnostic]
        extensions = [".txt"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.logs_agnostic]
        extensions = [".log"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "LineSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.rst_agnostic]
        extensions = [".rst"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        # Parameters for agnostic chunking methods
        [semantiscan.chunking.parameters.RecursiveSplitter]
        chunk_size = 1000  # Target chunk size in characters
        chunk_overlap = 150  # Overlap between chunks to preserve context

        [semantiscan.chunking.parameters.LineSplitter]
        lines_per_chunk = 50  # Number of lines per chunk

        [semantiscan.chunking.parameters.SubChunker]
        chunk_size = 500  # Chunk size for sub-chunking oversized chunks
        chunk_overlap = 50  # Overlap for sub-chunking

        # --- Ingestion Configuration ---
        # Controls the ingestion pipeline behavior
        [semantiscan.ingestion]
        embedding_workers = 4  # Number of parallel workers for embedding generation
        batch_size = 100  # Number of chunks to process in parallel batches

        # Git-aware ingestion modes (advanced feature)
        [semantiscan.ingestion.git]
        enabled = false  # Enable Git-aware ingestion tracking
        default_ref = "main"  # Default branch to ingest
        ingestion_mode = "snapshot"  # Modes: "snapshot" | "historical" | "historical_delta" | "incremental"
        historical_start_ref = ""  # Starting ref for historical ingestion modes
        enable_commit_analysis = false  # Extract commit messages, authors, etc.
        enable_commit_llm_analysis = false  # Use LLM to analyze/summarize commit messages
        commit_llm_provider_key = ""  # LLM provider key for commit analysis
        commit_llm_prompt_template = ""  # Custom prompt template for commit analysis
        commit_message_filter_regex = []  # Regex patterns to filter out certain commit messages

        # --- LLM Configuration (for query answering) ---
        # NOTE: In Phase 2, semantiscan will use llmcore's LLM providers directly.
        # This section is for query answering and advanced features like query rewriting.
        [semantiscan.llm]
        default_provider = "ollama_llama3"  # Key referencing llmcore's provider
        prompt_template_path = ""  # Optional: Custom prompt template file path
        enable_query_rewriting = false  # Enable LLM-based query expansion
        query_rewrite_provider_key = ""  # LLM provider for query rewriting
        show_sources_in_text = true  # Include source references in text responses
        tokenizer_name = ""  # Optional: Specific tokenizer for LLM context estimation
        context_buffer = 200  # Reserved tokens for prompt formatting

        # Reference to llmcore providers (will be resolved at runtime)
        [semantiscan.llm.providers.ollama_llama3]
        provider = "ollama"  # Maps to llmcore's ollama provider
        model_name = "llama3:8b"
        base_url = "http://localhost:11434"
        api_key_env = ""  # Not needed for Ollama
        tokenizer_name = ""
        context_buffer = 250
        # Parameters passed directly to the LLM API
        [semantiscan.llm.providers.ollama_llama3.parameters]
        temperature = 0.5
        num_ctx = 4096
        top_p = 0.9

        [semantiscan.llm.providers.openai_gpt4]
        provider = "openai"
        model_name = "gpt-4"
        base_url = ""  # Optional: Custom endpoint
        api_key_env = "OPENAI_API_KEY"
        tokenizer_name = "cl100k_base"
        context_buffer = 200
        [semantiscan.llm.providers.openai_gpt4.parameters]
        temperature = 0.2
        max_tokens = 4000

        [semantiscan.llm.providers.openai_gpt4_turbo]
        provider = "openai"
        model_name = "gpt-4-turbo-preview"
        base_url = ""
        api_key_env = "OPENAI_API_KEY"
        tokenizer_name = "cl100k_base"
        context_buffer = 200
        [semantiscan.llm.providers.openai_gpt4_turbo.parameters]
        temperature = 0.2
        max_tokens = 4000

        # --- Retrieval Configuration ---
        # Controls how semantiscan retrieves relevant chunks during RAG queries.
        [semantiscan.retrieval]
        top_k = 10  # Number of most relevant chunks to retrieve
        enable_hybrid_search = false  # Use hybrid search (semantic + keyword BM25)
        bm25_k1 = 1.5  # BM25 parameter: term frequency saturation
        bm25_b = 0.75  # BM25 parameter: length normalization
        enrich_with_external_metadata = false  # Include file metadata from external store in retrieval

        # --- File Discovery Configuration ---
        # Controls which files are included/excluded during ingestion.
        [semantiscan.discovery]
        use_gitignore = true  # Respect .gitignore rules during file scanning
        excluded_dirs = [  # Directories to always skip (even if not in .gitignore)
            "__pycache__",
            "node_modules",
            ".git",
            "venv",
            ".venv",
            "build",
            "dist",
            ".pytest_cache",
            ".mypy_cache",
            "htmlcov",
            ".tox",
            ".eggs",
            "*.egg-info",
            "target"  # Rust/Java build directories
        ]
        excluded_files = [".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.pyd"]  # Files to always skip

        # --- Logging Configuration ---
        # Semantiscan-specific logging (separate from llmcore's logging)
        [semantiscan.logging]
        log_level_console = "INFO"  # Console log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
        log_file_enabled = false  # Enable file logging for semantiscan
        log_directory = "~/.local/share/semantiscan/logs"  # Directory for log files
        log_filename_template = "semantiscan_{timestamp:%Y%m%d_%H%M%S}.log"  # Log filename template
        log_level_file = "DEBUG"  # File log level (more verbose than console)
        log_format = "%(asctime)s [%(levelname)-8s] %(name)-30s - %(message)s (%(filename)s:%(lineno)d)"

        # ==============================================================================
        # SANDBOX ADDITIONS FOR default_config.toml
        # ==============================================================================
        # INSTRUCTIONS: Append this content to the END of your existing default_config.toml
        # Do NOT replace the existing file - just add these sections at the bottom.
        # ==============================================================================

        # ==============================================================================
        # [agents] - Agent System Configuration
        # ==============================================================================
        [agents]

        # --- Agent Execution Settings ---
        # Default maximum iterations for agent loops
        max_iterations = 10

        # Default timeout for agent tasks (in seconds)
        default_timeout = 600

        # ==============================================================================
        # [agents.sandbox] - Sandbox Configuration for Agent Execution
        # ==============================================================================
        # This section configures the sandbox system for isolated agent code execution.
        # When enabled, agent tasks execute in Docker containers or VMs, ensuring
        # that agent-generated code NEVER runs on the host system.
        #
        # SECURITY: The sandbox is designed to prevent agents from accessing or
        # modifying the host system. All code execution happens in isolated environments.

        [agents.sandbox]
        # Sandbox mode determines which backend to use for isolation.
        # Possible Values: "docker", "vm", "hybrid"
        #   - docker: Use Docker containers (recommended for most use cases)
        #   - vm: Use SSH to a dedicated VM (for high-security scenarios)
        #   - hybrid: Try Docker first, fall back to VM if Docker unavailable
        # Default Value: "docker"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__MODE
        mode = "docker"

        # Enable fallback to VM if primary mode fails (only applies to hybrid mode)
        # Default Value: true
        fallback_enabled = true

        # --- Docker Sandbox Settings ---
        [agents.sandbox.docker]
        # Enable Docker sandbox support
        # Default Value: true
        enabled = true

        # Default Docker image for sandbox containers.
        # Should be a Python image with necessary tools.
        # Default Value: "python:3.11-slim"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__IMAGE
        image = "python:3.11-slim"

        # Whitelist of allowed Docker images (glob patterns supported).
        # Only images matching these patterns can be used.
        # Default Value: ["python:3.*-slim", "python:3.*-bookworm"]
        image_whitelist = [
            "python:3.*-slim",
            "python:3.*-bookworm",
            "llmcore-sandbox:*"
        ]

        # Docker label that grants FULL access level.
        # Containers with this label bypass tool restrictions.
        # Default Value: "llmcore.sandbox.full_access=true"
        full_access_label = "llmcore.sandbox.full_access=true"

        # Container name patterns that grant FULL access (glob patterns).
        # Default Value: []
        full_access_name_patterns = [
            "llmcore-trusted-*"
        ]

        # Memory limit for containers (Docker memory limit format).
        # Default Value: "1g"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__MEMORY_LIMIT
        memory_limit = "1g"

        # CPU limit for containers (number of CPU cores).
        # Default Value: 2.0
        cpu_limit = 2.0

        # Default timeout for operations in seconds.
        # Default Value: 600
        timeout_seconds = 600

        # Enable network access in containers.
        # WARNING: Enabling network access reduces isolation security.
        # Default Value: false
        network_enabled = false

        # Docker host URL (for remote Docker daemons).
        # Leave empty for local Docker socket.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__HOST
        # docker_host = "tcp://192.168.1.100:2375"

        # --- VM Sandbox Settings ---
        [agents.sandbox.vm]
        # Enable VM sandbox support (requires SSH access to a VM).
        # Default Value: false
        enabled = false

        # VM host address.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VM__HOST
        host = ""

        # SSH port.
        # Default Value: 22
        port = 22

        # SSH username.
        # Default Value: "agent"
        username = "agent"

        # Path to SSH private key file.
        # WARNING: Use SSH agent or environment variable in production.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VM__PRIVATE_KEY_PATH
        private_key_path = ""

        # Use SSH agent for authentication instead of key file.
        # Default Value: false
        use_ssh_agent = false

        # List of hosts that grant FULL access level.
        # VMs in this list bypass tool restrictions.
        # Default Value: []
        full_access_hosts = []

        # Default timeout for operations in seconds.
        # Default Value: 600
        timeout_seconds = 600

        # Working directory on the VM.
        # Default Value: "/tmp/llmcore_sandbox"
        working_directory = "/tmp/llmcore_sandbox"

        # --- Volume Mount Settings ---
        [agents.sandbox.volumes]
        # Host path for shared data that persists across sandbox instances.
        # This directory is mounted read-write in sandboxes.
        # Default Value: "~/.llmcore/agent_share"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VOLUMES__SHARE_PATH
        share_path = "~/.llmcore/agent_share"

        # Host path for agent output files.
        # Files saved here persist after sandbox cleanup.
        # Default Value: "~/.llmcore/agent_outputs"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VOLUMES__OUTPUTS_PATH
        outputs_path = "~/.llmcore/agent_outputs"

        # --- Tool Access Control ---
        [agents.sandbox.tools]
        # List of tools allowed for RESTRICTED access level sandboxes.
        # FULL access sandboxes bypass this restriction.
        # Default Value: (all standard sandbox tools)
        allowed = [
            "execute_shell",
            "execute_python",
            "save_file",
            "load_file",
            "replace_in_file",
            "append_to_file",
            "list_files",
            "file_exists",
            "delete_file",
            "create_directory",
            "get_state",
            "set_state",
            "list_state",
            "get_sandbox_info",
            "get_recorded_files",
            # Core agent tools
            "semantic_search",
            "episodic_search",
            "calculator",
            "finish",
            "human_approval"
        ]

        # List of tools explicitly denied for all access levels.
        # These tools are never available, even for FULL access.
        # Default Value: []
        denied = [
            # Example dangerous tools that should never be allowed:
            # "sudo_execute",
            # "raw_socket",
            # "install_system_package"
        ]

        # --- Output Tracking Settings ---
        [agents.sandbox.output_tracking]
        # Enable persistent tracking of agent outputs.
        # Default Value: true
        enabled = true

        # Maximum number of execution log entries per run.
        # Default Value: 1000
        max_log_entries = 1000

        # Maximum age of runs to keep (in days). 0 = keep forever.
        # Default Value: 30
        max_run_age_days = 30

        # Maximum number of runs to keep. 0 = no limit.
        # Default Value: 100
        max_runs = 100
