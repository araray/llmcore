# ==============================================================================
# LLMCore Default Configuration
# ==============================================================================
# This is the default configuration file for the LLMCore library.
# It defines sensible defaults for all configurable parameters.
# Users can override these settings via:
#   1. User config file: ~/.config/llmcore/config.toml
#   2. Custom config file: Specified via LLMCore.create(config_file_path=...)
#   3. Environment variables: Prefixed with LLMCORE_ (e.g., LLMCORE_DEFAULT_PROVIDER)
#   4. Direct overrides: Passed via LLMCore.create(config_overrides=...)

# ==============================================================================
# [llmcore] - Core Library Settings
# ==============================================================================
[llmcore]

# --- Default LLM Provider ---
# What it does: Specifies which provider instance (from [providers] below) to use
#               by default when no provider is explicitly chosen.
# Purpose: Allows you to switch between different LLM backends quickly.
# Possible Values: The name of any configured provider instance (e.g., "ollama",
#                  "openai", "anthropic", "gemini").
# Default Value: "ollama"
# Environment Variable: LLMCORE_DEFAULT_PROVIDER
default_provider = "ollama"

# --- Default Embedding Model ---
# What it does: Specifies which embedding model to use for RAG operations.
# Purpose: Required for generating vector embeddings of text chunks in RAG.
# Possible Values:
#   - For local models (Sentence Transformers): A model name from the Hugging Face
#     model hub (e.g., "all-MiniLM-L6-v2", "all-mpnet-base-v2").
#     The model will be downloaded automatically.
#   - For service-based models: A string in the format "provider:model_name"
#     (e.g., "openai:text-embedding-3-small", "google:models/embedding-001").
# Default Value: "all-MiniLM-L6-v2"
# Environment Variable: LLMCORE_DEFAULT_EMBEDDING_MODEL
default_embedding_model = "all-MiniLM-L6-v2"

# --- Logging Verbosity ---
# What it does: Configures the logging verbosity for the entire `llmcore` library.
# Purpose: Control the amount of diagnostic information emitted by the library.
# Possible Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
# Default Value: "INFO"
# Environment Variable: LLMCORE_LOG_LEVEL
log_level = "INFO"

# --- Raw Payload Logging (for Debugging) ---
# What it does: If true, `llmcore` will log the exact raw JSON request and
#               response payloads exchanged with the underlying LLM provider APIs.
#               This logging occurs at the `DEBUG` log level.
# Purpose: An indispensable tool for advanced troubleshooting. It allows you to
#          inspect the precise data being sent and received, which is essential
#          for diagnosing provider errors or understanding unexpected behavior.
# Possible Options: true, false
# Default Value: false
# Environment Variable: LLMCORE_LOG_RAW_PAYLOADS
log_raw_payloads = false

# --- Administrative API Key ---
# What it does: Defines the secret key required for accessing administrative endpoints
#               such as live configuration reloading. This provides a separate,
#               high-privilege authentication layer independent of tenant API keys.
# Purpose: Secures administrative operations that could impact the entire platform,
#          ensuring only authorized administrators can perform sensitive actions
#          like configuration reloads.
# Security: This key should be treated as a high-privilege secret. In production,
#           ALWAYS set this via the environment variable rather than in config files.
# Default Value: "" (empty - admin endpoints will be disabled)
# Environment Variable: LLMCORE_ADMIN_API_KEY
# Example: admin_api_key = "admin_llmk_abc123def456ghi789jkl"
admin_api_key = ""

# ==============================================================================
# [providers] - LLM Provider Configurations
# ==============================================================================
# This section is where all individual LLM provider instances are defined.
# You can define multiple instances of the same provider type by giving them
# unique names (e.g., [providers.my_openai_clone]) and setting `type = "openai"`.
[providers]

  # --- OpenAI Provider ---
  # For official OpenAI models and any OpenAI-compatible API (e.g., Groq, Anyscale).
  [providers.openai]
  # API Key: It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_PROVIDERS__OPENAI__API_KEY or OPENAI_API_KEY
  # api_key = "sk-..."

  # Base URL: Override for proxies or compatible APIs.
  # Example for Groq: base_url = "https://api.groq.com/openai/v1"
  # base_url = "https://api.openai.com/v1"

  # Default model for this provider instance.
  default_model = "gpt-4o"

  # Request timeout in seconds.
  timeout = 60

  # --- Anthropic Provider ---
  # For Claude models.
  [providers.anthropic]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__ANTHROPIC__API_KEY or ANTHROPIC_API_KEY
  # api_key = "sk-ant-..."

  # Default model for this provider instance.
  default_model = "claude-3-opus-20240229"
  timeout = 60

  # --- Ollama Provider ---
  # For locally running models via Ollama.
  [providers.ollama]
  # Host URL of the running Ollama server.
  # The official `ollama` Python library uses this parameter.
  # host = "http://localhost:11434"

  # Default local model to use. Ensure you have run `ollama pull <model_name>`.
  default_model = "llama3"

  # Longer timeout is recommended for local models.
  timeout = 120

  # Tokenizer for token counting. `tiktoken_cl100k_base` is a good default.
  # Use 'char_div_4' as a rough fallback if tiktoken causes issues with a model.
  # tokenizer = "tiktoken_cl100k_base"

  # --- Google Gemini Provider ---
  # For Gemini models.
  [providers.gemini]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__GEMINI__API_KEY or GOOGLE_API_KEY
  # api_key = "..."

  # Default model for this provider instance.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60

  # Optional: Configure Google AI's safety filters.
  # This table is passed directly to the `google-genai` SDK.
  # [providers.gemini.safety_settings]
  # HARM_CATEGORY_HARASSMENT = "BLOCK_NONE"
  # HARM_CATEGORY_HATE_SPEECH = "BLOCK_NONE"


# ==============================================================================
# [storage] - Persistence Configurations
# ==============================================================================
# This section governs where and how persistent data, such as conversation
# history (session memory) and RAG documents (semantic memory), is stored.
[storage]

  # --- Session & Episodic Memory Storage ---
  # Configures the backend for storing conversation history and agent episodes.
  [storage.session]
  # Type: Determines the storage backend.
  #   - "json": Simple, human-readable. Good for low-concurrency apps.
  #   - "sqlite": (Default) Excellent for single-process apps and local dev.
  #   - "postgres": Recommended for production, especially with multiple workers.
  # Environment Variable: LLMCORE_STORAGE__SESSION__TYPE
  type = "sqlite"

  # Path: Filesystem path for "json" and "sqlite" backends.
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_STORAGE__SESSION__PATH
  path = "~/.llmcore/sessions.db"

  # Database URL: Full connection URL for "postgres".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__SESSION__DB_URL
  # Example: db_url = "postgresql://user:password@host:port/database_name"
  db_url = ""

  # --- Vector Storage for RAG (Semantic Memory) ---
  # Configures the vector store for RAG documents and their embeddings.
  [storage.vector]
  # Type: Determines the vector database backend.
  #   - "chromadb": (Default) Easy to set up for local development.
  #   - "pgvector": Excellent for production if you already use PostgreSQL.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__TYPE
  type = "chromadb"

  # Default Collection: A namespace for RAG documents. Allows you to maintain
  # multiple distinct knowledge bases in the same vector store.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DEFAULT_COLLECTION
  default_collection = "llmcore_default_rag"

  # Path: Filesystem path for file-based vector stores like "chromadb".
  # Environment Variable: LLMCORE_STORAGE__VECTOR__PATH
  path = "~/.llmcore/chroma_db"

  # Database URL: Connection URL for "pgvector".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DB_URL
  db_url = ""


# ==============================================================================
# [embedding] - Vectorization Model Configurations
# ==============================================================================
# This section configures the models responsible for converting text into
# numerical vectors (embeddings) for the RAG system. The global
# `llmcore.default_embedding_model` setting determines which model to use.
# If that identifier contains a colon (e.g., "openai:text-embedding-3-small"),
# the part before the colon tells the system which configuration block below
# to consult for specific settings like API keys.
[embedding]

  [embedding.openai]
  # API Key for OpenAI embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__OPENAI__API_KEY
  # api_key = ""
  default_model = "text-embedding-3-small"
  # base_url = "..." # Optional proxy URL

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__GOOGLE__API_KEY
  # api_key = ""
  default_model = "models/embedding-001"

  [embedding.ollama]
  # Host URL for the Ollama server for embeddings.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings. Ensure it's pulled locally.
  # Examples: "mxbai-embed-large", "nomic-embed-text"
  default_model = "mxbai-embed-large"
  timeout = 60

  [embedding.sentence_transformer]
  # Additional configuration for local `sentence-transformers` models.
  # The model itself is specified in `llmcore.default_embedding_model`.
  # Device to run the model on. If not set, the library will auto-detect.
  # Options: "cpu", "cuda", "mps"
  # device = "cpu"


# ==============================================================================
# [context_management] - Advanced Prompt Control
# ==============================================================================
# This section is the control center for prompt engineering. It governs the
# sophisticated, multi-stage process of assembling the final context that is
# sent to the LLM. The system first prioritizes what to INCLUDE, and only then
# decides what to TRUNCATE if the model's token limit is exceeded.
[context_management]

  # --- Inclusion Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               components are added to the prompt.
  # Purpose: Gives fine-grained control over the final prompt's structure.
  # Valid Components: "system_history", "explicitly_staged", "user_items_active",
  #                   "history_chat", "final_user_query".
  # Default: "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"
  inclusion_priority = "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"

  # --- Truncation Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               types are removed to meet token limits.
  # Purpose: Controls what information is sacrificed when the context is too long.
  # Valid Components: "history_chat", "user_items_active", "rag_in_query",
  #                   "explicitly_staged".
  # Default: "history_chat,user_items_active,rag_in_query,explicitly_staged"
  truncation_priority = "history_chat,user_items_active,rag_in_query,explicitly_staged"

  # --- RAG Prompt Template ---
  # What it does: The template used to format the final query when RAG is enabled.
  #               It will be formatted with `{context}` (retrieved documents) and
  #               `{question}` (the user's query).
  # Purpose: To instruct the LLM on how to use the provided RAG context.
  default_prompt_template = """You are an AI assistant specialized in answering questions about codebases
  based on provided context.
  Use ONLY the following pieces of retrieved context to answer the user's question.
  If the answer is not found in the context, state that you cannot answer based on the provided information.
  Do not make up an answer or use external knowledge. Keep the answer concise and relevant to the question.
  Include relevant source file paths and line numbers if possible, based *only* on the provided context metadata.

  Context:
  ---------------------
  {context}
  ---------------------

  Question: {question}

  Answer:"""

    # Optional path to a custom prompt template file. Overrides the string above.
    prompt_template_path = ""

    # Default number of documents to retrieve for RAG.
    rag_retrieval_k = 3

    # Number of tokens to leave free in the context window for the model's response.
    reserved_response_tokens = 500

    # Minimum number of recent chat messages to try to keep during truncation.
    minimum_history_messages = 1

    # Prioritizes keeping the N most recent user messages (and their preceding
    # assistant responses) during truncation.
    user_retained_messages_count = 5

    # A safeguard character limit for a single user-provided context item.
    # The default is very large to accommodate large documents.
    max_chars_per_user_item = 40000000


    # ==============================================================================
    # Step 1.5: Semantiscan Configuration Section
    # ==============================================================================
    # This section should be added to: src/llmcore/config/default_config.toml
    #
    # Purpose: Unified configuration for the semantiscan RAG/ingestion engine.
    # After Phase 1, semantiscan will no longer maintain its own config.toml;
    # instead, it will receive configuration from llmcore via this section.
    #
    # References:
    # - semantiscan/config/loaders.py (DEFAULT_CONFIG)
    # - semantiscan/config/models.py (AppConfig schema)
    # - Resurrection Plan Section VI, Task 1.3
    # ==============================================================================

    [semantiscan]
    # Top-level semantiscan enablement flag (for future use)
    enabled = true

        # --- Database Configuration (Vector Store) ---
        # Defines where semantiscan stores embedded code chunks for RAG retrieval.
        # IMPORTANT: For unified storage, this should match llmcore's [storage.vector.path]
        [semantiscan.database]
        type = "chromadb"  # Currently only ChromaDB is supported by semantiscan
        # Default path - should be synchronized with llmcore's vector storage path
        path = "~/.llmcore/chroma_db"
        collection_name = "default_semantiscan"  # Default collection for ingested code

        # --- Metadata Store Configuration ---
        # Optional SQLite database for tracking ingestion history, Git metadata, etc.
        [semantiscan.metadata_store]
        enabled = false  # Disabled by default; enable for Git-aware ingestion tracking
        path = "~/.local/share/semantiscan/metadata.db"
        # Table names (advanced configuration - usually no need to change)
        scans_table = "scans"
        chunks_table = "chunks"
        commit_log_table = "commit_log"
        file_history_table = "file_history"

        # --- Embedding Configuration ---
        # Defines which embedding models are available for chunking ingestion.
        # semantiscan will use llmcore's EmbeddingManager for actual embedding generation.
        [semantiscan.embeddings]
        default_model = "sentence_transformer_local"  # Default embedding model key

        # Define available embedding models (keys reference llmcore's [embedding.*] sections)
        [semantiscan.embeddings.models.sentence_transformer_local]
        provider = "sentence-transformers"  # Maps to llmcore's sentence_transformer provider
        model_name = "all-MiniLM-L6-v2"  # Default local model
        device = "cpu"  # "cpu" | "cuda" | "mps" (for M1/M2 Macs)
        # Optional: Override context/request limits for this model
        max_request_tokens = 8000  # Max tokens per embedding batch request

        [semantiscan.embeddings.models.openai_ada]
        provider = "openai"  # Maps to llmcore's openai embedding provider
        model_name = "text-embedding-ada-002"
        api_key_env = "OPENAI_API_KEY"  # Reference to environment variable
        max_request_tokens = 8000

        [semantiscan.embeddings.models.openai_large]
        provider = "openai"
        model_name = "text-embedding-3-large"
        api_key_env = "OPENAI_API_KEY"
        max_request_tokens = 8000

        # --- Chunking Configuration ---
        # Defines how different file types are parsed and chunked during ingestion.
        [semantiscan.chunking]
        default_strategy = "AgnosticRecursive"  # Fallback for unknown file types

        # ANTLR-based code parsers (requires ANTLR runtime and generated parsers)
        [semantiscan.chunking.strategies.antlr]
        # Python code parsing
        python = {
            extensions = [".py"],
            grammar = "Python3",  # ANTLR grammar name (must be pre-built)
            entry_points = ["funcdef", "async_funcdef", "classdef"]  # AST nodes to extract
        }
        # Java code parsing
        java = {
            extensions = [".java"],
            grammar = "Java",
            entry_points = ["methodDeclaration", "classDeclaration", "interfaceDeclaration"]
        }
        # Additional language parsers can be added here
        # cpp = { extensions = [".cpp", ".cc", ".h"], grammar = "CPP14", entry_points = ["functionDefinition", "classSpecifier"] }
        # go = { extensions = [".go"], grammar = "Golang", entry_points = ["functionDecl", "methodDecl"] }

        # Format-specific parsers (YAML, JSON, TOML, XML, etc.)
        [semantiscan.chunking.strategies.format]
        yaml = { extensions = [".yaml", ".yml"], parser = "PyYAML" }
        json = { extensions = [".json"], parser = "json" }
        toml = { extensions = [".toml"], parser = "tomli" }
        xml = { extensions = [".xml"], parser = "xml.etree.ElementTree" }

        # Agnostic text splitters (for markdown, plain text, logs, etc.)
        [semantiscan.chunking.strategies.agnostic]
        markdown = { extensions = [".md", ".markdown"], method = "RecursiveSplitter" }
        text = { extensions = [".txt"], method = "RecursiveSplitter" }
        logs = { extensions = [".log"], method = "LineSplitter" }
        rst = { extensions = [".rst"], method = "RecursiveSplitter" }

        # Parameters for agnostic chunking methods
        [semantiscan.chunking.parameters.RecursiveSplitter]
        chunk_size = 1000  # Target chunk size in characters
        chunk_overlap = 150  # Overlap between chunks to preserve context

        [semantiscan.chunking.parameters.LineSplitter]
        lines_per_chunk = 50  # Number of lines per chunk

        [semantiscan.chunking.parameters.WholeFile]
        # No parameters - just ingests the entire file as one chunk

        # --- Ingestion Configuration ---
        # Controls the ingestion pipeline behavior
        [semantiscan.ingestion]
        embedding_workers = 4  # Number of parallel workers for embedding generation
        batch_size = 100  # Number of chunks to process in parallel

        # Git-aware ingestion modes (advanced feature)
        [semantiscan.ingestion.git]
        enabled = false  # Enable Git-aware ingestion tracking
        default_ref = "main"  # Default branch to ingest
        enable_commit_analysis = false  # Extract commit messages, authors, etc.
        # Ingestion modes: "snapshot" | "historical" | "historical_delta" | "incremental"
        ingestion_mode = "snapshot"
        historical_start_ref = null  # Starting ref for historical ingestion

        # --- LLM Configuration (for query answering) ---
        # NOTE: In Phase 2, semantiscan will use llmcore's LLM providers directly.
        # This section is for future query rewriting and advanced features.
        [semantiscan.llm]
        default_provider = "ollama_llama3"  # Key referencing llmcore's provider
        prompt_template_path = null  # Optional: Custom prompt template file path
        enable_query_rewriting = false  # Enable LLM-based query expansion
        query_rewrite_provider_key = null  # LLM provider for query rewriting
        show_sources_in_text = true  # Include source references in text responses

        # Reference to llmcore providers (will be resolved at runtime)
        [semantiscan.llm.providers.ollama_llama3]
        provider = "ollama"  # Maps to llmcore's ollama provider
        model_name = "llama3:8b"
        base_url = "http://localhost:11434"
        parameters = { temperature = 0.5, num_ctx = 4096 }
        context_buffer = 250  # Reserved tokens for prompt formatting

        [semantiscan.llm.providers.openai_gpt4]
        provider = "openai"
        model_name = "gpt-4"
        api_key_env = "OPENAI_API_KEY"
        parameters = { temperature = 0.2, max_tokens = 4000 }

        # --- Retrieval Configuration ---
        # Controls how semantiscan retrieves relevant chunks during RAG queries.
        [semantiscan.retrieval]
        top_k = 10  # Number of most relevant chunks to retrieve
        enable_hybrid_search = false  # Use hybrid search (semantic + keyword BM25)
        bm25_k1 = 1.5  # BM25 parameter: term frequency saturation
        bm25_b = 0.75  # BM25 parameter: length normalization
        enrich_with_external_metadata = false  # Include file metadata in retrieval

        # --- File Discovery Configuration ---
        # Controls which files are included/excluded during ingestion.
        [semantiscan.discovery]
        use_gitignore = true  # Respect .gitignore rules during file scanning
        excluded_dirs = [  # Directories to always skip (even if not in .gitignore)
            "__pycache__",
            "node_modules",
            ".git",
            "venv",
            ".venv",
            "build",
            "dist",
            ".pytest_cache",
            ".mypy_cache",
            "htmlcov"
        ]
        excluded_files = [".DS_Store", "Thumbs.db"]  # Files to always skip

        # --- Logging Configuration ---
        # Semantiscan-specific logging (separate from llmcore's logging)
        [semantiscan.logging]
        log_level_console = "INFO"  # Console log level for semantiscan operations
        log_file_enabled = false  # Enable file logging for semantiscan
        log_directory = "~/.local/share/semantiscan/logs"
        log_filename_template = "semantiscan_{timestamp:%Y%m%d_%H%M%S}.log"
        log_level_file = "DEBUG"  # File log level (more verbose than console)
        log_format = "%(asctime)s [%(levelname)-8s] %(name)-30s - %(message)s (%(filename)s:%(lineno)d)"
