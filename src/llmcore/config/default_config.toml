# src/llmcore/config/default_config.toml
# Default configuration structure for LLMCore
# This file defines the standard settings and can be overridden by user configurations,
# environment variables, or direct overrides in code.

[llmcore]
# Default provider to use if not specified in API calls.
# Options: "openai", "anthropic", "ollama", "gemini", or any custom provider name.
default_provider = "ollama"

# Default embedding model for RAG (Retrieval Augmented Generation).
# This can be a local path to a sentence-transformers model (e.g., "all-MiniLM-L6-v2")
# or an identifier for a service-based model (e.g., "openai:text-embedding-ada-002", "google:models/embedding-001").
default_embedding_model = "all-MiniLM-L6-v2" # A common sentence-transformer model

# Log level for the library.
# Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
log_level = "INFO"


# --- Provider Configurations ---
# Each provider has its own section under [providers.<provider_name>].
# API keys are best set via environment variables (e.g., LLMCORE_PROVIDERS_OPENAI_API_KEY).
[providers]

  [providers.openai]
  # API Key: Recommended to use environment variable LLMCORE_PROVIDERS_OPENAI_API_KEY
  api_key = "" # Leave empty to rely on environment variable
  default_model = "gpt-4-turbo"
  timeout = 60 # Timeout in seconds for API calls

  [providers.anthropic]
  # API Key: Recommended to use environment variable LLMCORE_PROVIDERS_ANTHROPIC_API_KEY
  api_key = "" # Leave empty to rely on environment variable
  default_model = "claude-3-opus-20240229"
  timeout = 60 # Timeout in seconds for API calls

  [providers.ollama]
  base_url = "http://localhost:11434/api" # Base URL for the Ollama API
  default_model = "gemma3:4b" # Default model to use with Ollama
  timeout = 120 # Timeout in seconds for API calls
  # Optional: Specify tokenizer for Ollama models if default (tiktoken_cl100k_base) is not suitable.
  # Accurate token counting for Ollama can be tricky. Options:
  # 'tiktoken_cl100k_base': (Default) Good general-purpose tokenizer (used by GPT-3.5/4).
  # 'tiktoken_p50k_base': Another tiktoken option.
  # 'char_div_4': A very rough estimate (characters / 4). Use if tiktoken is problematic.
  # tokenizer = "tiktoken_cl100k_base"

  [providers.gemini]
  # API Key: Recommended to use environment variable LLMCORE_PROVIDERS_GEMINI_API_KEY
  api_key = "" # Leave empty to rely on environment variable
  default_model = "gemini-2.0-flash-lite"
  timeout = 60 # Timeout in seconds for API calls
  # Add other Gemini specific settings if needed (e.g., safety settings).
  # Example: safety_settings = { HARM_CATEGORY_SEXUALLY_EXPLICIT = "BLOCK_NONE" }


# --- Storage Configurations ---
# Defines where and how session history and vector embeddings are stored.
[storage]

  # Session storage configuration: for conversation history.
  [storage.session]
  # Type: 'json', 'sqlite', 'postgres'
  type = "sqlite"

  # Path for file-based storage (json, sqlite).
  # '~' will be expanded to the user's home directory.
  path = "~/.llmcore/sessions.db" # For SQLite, this is the DB file. For JSON, this is the directory.

  # Connection URL for database storage (e.g., postgres).
  # Recommended to use environment variable: LLMCORE_STORAGE_SESSION_DB_URL
  # Example: db_url = "postgresql://user:pass@host:port/dbname"
  db_url = ""

  # Optional table name for database storage.
  # table_name = "llmcore_sessions"

  # Vector storage configuration: for RAG documents and embeddings.
  [storage.vector]
  # Type: 'chromadb', 'pgvector' (PostgreSQL with pgvector extension)
  type = "chromadb"

  # Default collection name used for RAG if not specified in API calls.
  # Collections in vector stores are reusable across different sessions or applications.
  default_collection = "llmcore_default_rag"

  # Path for file-based vector stores (e.g., chromadb persistent client).
  # '~' will be expanded to the user's home directory.
  path = "~/.llmcore/chroma_db"

  # Connection URL for database vector stores (e.g., pgvector).
  # Recommended to use environment variable: LLMCORE_STORAGE_VECTOR_DB_URL
  # Example: db_url = "postgresql://user:pass@host:port/dbname"
  db_url = ""

  # Optional table name for database vector storage.
  # table_name = "llmcore_vectors"


# --- Embedding Model Configurations ---
# Configures embedding models used for RAG.
[embedding]
  # Configuration for specific embedding models if they require special setup,
  # like API keys for service-based ones. The 'llmcore.default_embedding_model'
  # setting determines which model is used by default.
  # If 'llmcore.default_embedding_model' is like "openai:text-embedding-3-small",
  # then settings from [embedding.openai] might be used.

  [embedding.openai]
  # API Key for OpenAI embeddings (if different from chat or if using only embeddings).
  # Recommended to use environment variable: LLMCORE_EMBEDDING_OPENAI_API_KEY
  # If empty and providers.openai.api_key is set, that might be reused by the provider logic.
  # api_key = ""
  # Default OpenAI model for embeddings.
  default_model = "text-embedding-3-small" # Example, check latest models

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Recommended to use environment variable: LLMCORE_EMBEDDING_GOOGLE_API_KEY
  # api_key = ""
  # Default Google AI model for embeddings.
  default_model = "models/embedding-001" # Example, check latest models

  # For local sentence-transformers, the model name/path is usually specified directly
  # in 'llmcore.default_embedding_model' (e.g., "all-MiniLM-L6-v2").
  # No specific section needed here unless you want to group related settings.
  # [embedding.sentence_transformer]
  # device = "cpu" # Example: "cuda", "cpu", "mps" (device to run model on)


# --- Context Management Configurations ---
# Defines strategies for how context is built and managed for LLM prompts.
[context_management]
  # Default number of documents to retrieve for RAG.
  rag_retrieval_k = 3

  # Strategy for combining RAG results with conversation history.
  # 'prepend_system': RAG context is added as part of a system message or preamble.
  # 'prepend_user': RAG context is added before the latest user message.
  rag_combination_strategy = "prepend_system"

  # Strategy for selecting history messages to fit within the token limit.
  # 'last_n_tokens': Prioritizes keeping the most recent messages that fit the token budget.
  # 'last_n_messages': Keeps a fixed number of the most recent messages (less precise for token limits).
  history_selection_strategy = "last_n_tokens"

  # Number of tokens to reserve for the LLM's response generation.
  # This ensures there's space for the model to actually write its answer.
  reserved_response_tokens = 500

  # Strategy for handling context overflow when total tokens (history + RAG + prompt) exceed the limit.
  # 'history': Truncate older history messages first.
  # 'rag': Truncate less relevant RAG documents first.
  truncation_priority = "history"

  # Minimum number of history messages (excluding system message) to try and keep during truncation.
  # This helps maintain some conversational flow even when context is tight.
  minimum_history_messages = 1
