# src/llmcore/config/default_config.toml
# Default configuration structure for LLMCore
# This file defines the standard settings and can be overridden by user configurations,
# environment variables, or direct overrides in code.

[llmcore]
# Default provider to use if not specified in API calls.
# This name should correspond to one of the sections under [providers].
# Example: default_provider = "my_openai_clone" if you define [providers.my_openai_clone]
default_provider = "ollama"

# Default embedding model for RAG (Retrieval Augmented Generation).
# This can be a local path to a sentence-transformers model (e.g., "all-MiniLM-L6-v2")
# or an identifier for a service-based model (e.g., "openai:text-embedding-ada-002", "google:models/embedding-001").
default_embedding_model = "all-MiniLM-L6-v2" # A common sentence-transformer model

# Log level for the library.
# Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
log_level = "INFO"


# --- Provider Configurations ---
# Each provider instance is defined in its own section under [providers].
# To use a specific base provider class (like OpenAIProvider), specify `type = "openai"`.
# API keys are best set via environment variables (e.g., LLMCORE_PROVIDERS_OPENAI__API_KEY for the default OpenAI instance,
# or LLMCORE_PROVIDERS__MY_CUSTOM_OPENAI_BACKEND__API_KEY for a custom one).
# However, they can also be set directly in these configuration blocks.
[providers]

  [providers.openai]
  # This section configures a provider instance named "openai" that uses the "openai" base type.
  # The 'type = "openai"' is implicit here because the section name matches a known type.
  # API Key: Can be set here, or via LLMCORE_PROVIDERS_OPENAI_API_KEY, or OPENAI_API_KEY.
  # api_key = "sk-your_openai_api_key_here"
  # base_url = "https://api.openai.com/v1" # Default OpenAI URL, can be overridden for proxies
  default_model = "gpt-4o" # Updated default
  timeout = 60 # Timeout in seconds for API calls
  # fallback_context_length = 128000 # In case it can't determine the model's context length, it assumes this is the maximum context length.

  # Example of a custom OpenAI-like provider configuration:
  # [providers.my_custom_openai_backend]
  # # Explicitly state that this custom backend uses the "openai" provider logic.
  # type = "openai"
  # # Specific API key for this custom backend.
  # # Can also be set via environment variable: LLMCORE_PROVIDERS_MY_CUSTOM_OPENAI_BACKEND_API_KEY
  # api_key = "your_custom_api_key_for_this_backend"
  # # The base URL for your OpenAI-compatible API.
  # base_url = "https://your.openai.like.api/v1"
  # default_model = "custom-model-name"
  # timeout = 120


  [providers.anthropic]
  # API Key: Can be set here, or via LLMCORE_PROVIDERS_ANTHROPIC_API_KEY, or ANTHROPIC_API_KEY.
  # api_key = "sk-ant-your_anthropic_api_key_here"
  default_model = "claude-3-opus-20240229"
# fallback_context_length = 128000 # In case it can't determine the model's context length, it assumes this is the maximum context length.
  timeout = 60 # Timeout in seconds for API calls

  [providers.ollama]
  # host = "http://localhost:11434" # Hostname/IP for the ollama library client (library default is usually fine)
  # base_url = "http://localhost:11434/api" # Base URL for direct API calls if provider used raw HTTP.
                                          # The official 'ollama' library uses 'host'.
  default_model = "llama3" # Updated default
  timeout = 120 # Timeout in seconds for API calls
  # fallback_context_length = 128000 # In case it can't determine the model's context length, it assumes this is the maximum context length.
  # Optional: Specify tokenizer for Ollama models if default (tiktoken_cl100k_base) is not suitable.
  # Accurate token counting for Ollama can be tricky. Options:
  # 'tiktoken_cl100k_base': (Default) Good general-purpose tokenizer (used by GPT-3.5/4).
  # 'tiktoken_p50k_base': Another tiktoken option.
  # 'char_div_4': A very rough estimate (characters / 4). Use if tiktoken is problematic.
  # tokenizer = "tiktoken_cl100k_base"

  [providers.gemini]
  # API Key: Can be set here, or via LLMCORE_PROVIDERS_GEMINI__API_KEY, or GOOGLE_API_KEY.
  # api_key = "your_google_ai_api_key_here"
  # fallback_context_length = 128000 # In case it can't determine the model's context length, it assumes this is the maximum context length.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60 # Timeout in seconds for API calls
  # Add other Gemini specific settings if needed (e.g., safety settings).
  # Example: safety_settings = { HARM_CATEGORY_SEXUALLY_EXPLICIT = "BLOCK_NONE" }


# --- Storage Configurations ---
# Defines where and how session history and vector embeddings are stored.
[storage]

  # Session storage configuration: for conversation history.
  [storage.session]
  # Type: 'json', 'sqlite', 'postgres'
  type = "sqlite"

  # Path for file-based storage (json, sqlite).
  # '~' will be expanded to the user's home directory.
  path = "~/.llmcore/sessions.db" # For SQLite, this is the DB file. For JSON, this is the directory.

  # Connection URL for database storage (e.g., postgres).
  # Recommended to use environment variable: LLMCORE_STORAGE_SESSION_DB_URL
  # Example: db_url = "postgresql://user:pass@host:port/dbname"
  db_url = ""

  # Optional table name for database storage.
  # table_name = "llmcore_sessions"

  # Vector storage configuration: for RAG documents and embeddings.
  [storage.vector]
  # Type: 'chromadb', 'pgvector' (PostgreSQL with pgvector extension)
  type = "chromadb"

  # Default collection name used for RAG if not specified in API calls.
  # Collections in vector stores are reusable across different sessions or applications.
  default_collection = "llmcore_default_rag"

  # Path for file-based vector stores (e.g., chromadb persistent client).
  # '~' will be expanded to the user's home directory.
  path = "~/.llmcore/chroma_db"

  # Connection URL for database vector stores (e.g., pgvector).
  # Recommended to use environment variable: LLMCORE_STORAGE_VECTOR_DB_URL
  # Example: db_url = "postgresql://user:pass@host:port/dbname"
  db_url = ""

  # Optional table name for database vector storage.
  # table_name = "llmcore_vectors"


# --- Embedding Model Configurations ---
# Configures embedding models used for RAG.
[embedding]
  # Configuration for specific embedding models if they require special setup,
  # like API keys for service-based ones. The 'llmcore.default_embedding_model'
  # setting determines which model is used by default.
  # If 'llmcore.default_embedding_model' is like "openai:text-embedding-3-small",
  # then settings from [embedding.openai] might be used.

  [embedding.openai]
  # API Key for OpenAI embeddings (if different from chat or if using only embeddings).
  # Recommended to use environment variable: LLMCORE_EMBEDDING_OPENAI_API_KEY
  # If empty and providers.openai.api_key is set, that might be reused by the provider logic.
  # api_key = ""
  # Default OpenAI model for embeddings.
  default_model = "text-embedding-3-small" # Example, check latest models
  # base_url = "..." # Can also be set if using a proxy for embeddings

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Recommended to use environment variable: LLMCORE_EMBEDDING_GOOGLE_API_KEY
  # api_key = ""
  # Default Google AI model for embeddings.
  default_model = "models/embedding-001" # Example, check latest models

  [embedding.ollama]
  # Host for the Ollama server if different from the main Ollama provider host for chat.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings (e.g., "mxbai-embed-large", "nomic-embed-text")
  # Ensure this model is pulled in your Ollama instance.
  default_model = "mxbai-embed-large"
  # timeout = 60

  # For local sentence-transformers, the model name/path is usually specified directly
  # in 'llmcore.default_embedding_model' (e.g., "all-MiniLM-L6-v2").
  # This section can provide additional parameters if needed by SentenceTransformerEmbedding.
  [embedding.sentence_transformer]
  # device = "cpu" # Example: "cuda", "cpu", "mps" (device to run model on)
  # model_name_or_path = "all-MiniLM-L6-v2" # This is usually taken from llmcore.default_embedding_model


# --- Context Management Configurations ---
# Defines strategies for how context is built and managed for LLM prompts.
[context_management]
  # Default number of documents to retrieve for RAG.
  rag_retrieval_k = 3

  # Strategy for combining RAG results with conversation history.
  # 'prepend_system': RAG context is added as part of a system message or preamble.
  # 'prepend_user': RAG context is added before the latest user message. (Not fully implemented yet)
  rag_combination_strategy = "prepend_system"

  # Strategy for selecting history messages to fit within the token limit.
  # 'last_n_tokens': Prioritizes keeping the most recent messages that fit the token budget.
  # 'last_n_messages': Keeps a fixed number of the most recent messages (less precise for token limits - Not implemented yet).
  history_selection_strategy = "last_n_tokens"

  # Number of tokens to reserve for the LLM's response generation.
  # This ensures there's space for the model to actually write its answer.
  reserved_response_tokens = 500

  # Strategy for handling context overflow when total tokens (history + RAG + prompt) exceed the limit.
  # The order defines which type of context is truncated first.
  # Options: 'history', 'rag', 'user_items'.
  # Example: "history,rag,user_items" means history is truncated first, then RAG, then user-added items.
  truncation_priority = "history,rag,user_items"

  # Minimum number of history messages (excluding system message) to try and keep during truncation.
  # This helps maintain some conversational flow even when context is tight.
  minimum_history_messages = 1

  # Number of most recent user messages (and their preceding assistant messages) to prioritize keeping in context.
  # Set to 0 to disable this specific retention and rely solely on last_n_tokens/last_n_messages for history.
  user_retained_messages_count = 5

  # If true, user-added context items (from /context add) are prioritized over RAG results
  # when initially filling the context budget (before overall truncation).
  prioritize_user_context_items = true

  # Maximum character length for any single user-added context item (text, file, pinned RAG).
  # Items exceeding this will be truncated by the ContextManager.
  # Approximately 10,000 tokens (1 token ~ 4 chars).
  max_chars_per_user_item = 40000
