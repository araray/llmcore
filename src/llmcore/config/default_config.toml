# ==============================================================================
# LLMCore Default Configuration
# ==============================================================================
# This is the default configuration file for the LLMCore library.
# It defines sensible defaults for all configurable parameters.
# Users can override these settings via:
#   1. User config file: ~/.config/llmcore/config.toml
#   2. Custom config file: Specified via LLMCore.create(config_file_path=...)
#   3. Environment variables: Prefixed with LLMCORE_ (e.g., LLMCORE_DEFAULT_PROVIDER)
#   4. Direct overrides: Passed via LLMCore.create(config_overrides=...)

# ==============================================================================
# [llmcore] - Core Library Settings
# ==============================================================================
[llmcore]

# --- Default LLM Provider ---
# What it does: Specifies which provider instance (from [providers] below) to use
#               by default when no provider is explicitly chosen.
# Purpose: Allows you to switch between different LLM backends quickly.
# Possible Values: The name of any configured provider instance (e.g., "ollama",
#                  "openai", "anthropic", "gemini").
# Default Value: "ollama"
# Environment Variable: LLMCORE_DEFAULT_PROVIDER
default_provider = "ollama"

# --- Default Embedding Model ---
# What it does: Specifies which embedding model to use for RAG operations.
# Purpose: Required for generating vector embeddings of text chunks in RAG.
# Possible Values:
#   - For local models (Sentence Transformers): A model name from the Hugging Face
#     model hub (e.g., "all-MiniLM-L6-v2", "all-mpnet-base-v2").
#     The model will be downloaded automatically.
#   - For service-based models: A string in the format "provider:model_name"
#     (e.g., "openai:text-embedding-3-small", "google:models/embedding-001").
# Default Value: "all-MiniLM-L6-v2"
# Environment Variable: LLMCORE_DEFAULT_EMBEDDING_MODEL
default_embedding_model = "all-MiniLM-L6-v2"

# --- Logging Verbosity ---
# What it does: Configures the logging verbosity for the entire `llmcore` library.
# Purpose: Control the amount of diagnostic information emitted by the library.
# Possible Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
# Default Value: "INFO"
# Environment Variable: LLMCORE_LOG_LEVEL
log_level = "INFO"

# --- Raw Payload Logging (for Debugging) ---
# What it does: If true, `llmcore` will log the exact raw JSON request and
#               response payloads exchanged with the underlying LLM provider APIs.
#               This logging occurs at the `DEBUG` log level.
# Purpose: An indispensable tool for advanced troubleshooting. It allows you to
#          inspect the precise data being sent and received, which is essential
#          for diagnosing provider errors or understanding unexpected behavior.
# Possible Options: true, false
# Default Value: false
# Environment Variable: LLMCORE_LOG_RAW_PAYLOADS
log_raw_payloads = false

# --- Administrative API Key ---
# What it does: Defines the secret key required for accessing administrative endpoints
#               such as live configuration reloading. This provides a separate,
#               high-privilege authentication layer independent of tenant API keys.
# Purpose: Secures administrative operations that could impact the entire platform,
#          ensuring only authorized administrators can perform sensitive actions
#          like configuration reloads.
# Security: This key should be treated as a high-privilege secret. In production,
#           ALWAYS set this via the environment variable rather than in config files.
# Default Value: "" (empty - admin endpoints will be disabled)
# Environment Variable: LLMCORE_ADMIN_API_KEY
# Example: admin_api_key = "admin_llmk_abc123def456ghi789jkl"
admin_api_key = ""

# =============================================================================
# UNIFIED LOGGING CONFIGURATION
# =============================================================================
# This section controls logging for ALL components: llmcore, llmchat,
# semantiscan, and confy. Configuration here is the single source of truth.

[logging]
# Console logging - set to false to suppress ALL console output
console_enabled = false
console_level = "WARNING"        # Minimum level for console (if enabled)
console_format = "%(levelname)s - %(message)s"

# File logging - always recommended to keep enabled
file_enabled = true
file_level = "DEBUG"             # Capture full detail in files
file_directory = "~/.local/share/llmcore/logs"
file_name_pattern = "{app}_{timestamp:%Y%m%d_%H%M%S}.log"
file_format = "%(asctime)s [%(levelname)-8s] %(name)-30s - %(message)s (%(filename)s:%(lineno)d)"

# Per-component log levels (override the handlers' levels for specific loggers)
[logging.components]
llmchat = "INFO"
llmcore = "INFO"
semantiscan = "INFO"
confy = "WARNING"
# Suppress noisy third-party libraries
urllib3 = "WARNING"
httpx = "WARNING"
httpcore = "WARNING"
asyncio = "WARNING"
aiosqlite = "WARNING"
chromadb = "WARNING"
posthog = "ERROR"

# ==============================================================================
# [providers] - LLM Provider Configurations
# ==============================================================================
# This section is where all individual LLM provider instances are defined.
# You can define multiple instances of the same provider type by giving them
# unique names (e.g., [providers.my_openai_clone]) and setting `type = "openai"`.
[providers]

  # --- OpenAI Provider ---
  # For official OpenAI models and any OpenAI-compatible API (e.g., Groq, Anyscale).
  [providers.openai]
  # API Key: It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_PROVIDERS__OPENAI__API_KEY or OPENAI_API_KEY
  # api_key = "sk-..."

  # Base URL: Override for proxies or compatible APIs.
  # Example for Groq: base_url = "https://api.groq.com/openai/v1"
  # base_url = "https://api.openai.com/v1"

  # Default model for this provider instance.
  default_model = "gpt-4o"

  # Request timeout in seconds.
  timeout = 60

  # --- Anthropic Provider ---
  # For Claude models.
  [providers.anthropic]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__ANTHROPIC__API_KEY or ANTHROPIC_API_KEY
  # api_key = "sk-ant-..."

  # Default model for this provider instance.
  default_model = "claude-3-opus-20240229"
  timeout = 60

  # --- Ollama Provider ---
  # For locally running models via Ollama.
  [providers.ollama]
  # Host URL of the running Ollama server.
  # The official `ollama` Python library uses this parameter.
  # host = "http://localhost:11434"

  # Default local model to use. Ensure you have run `ollama pull <model_name>`.
  default_model = "llama3"

  # Longer timeout is recommended for local models.
  timeout = 120

  # Tokenizer for token counting. `tiktoken_cl100k_base` is a good default.
  # Use 'char_div_4' as a rough fallback if tiktoken causes issues with a model.
  # tokenizer = "tiktoken_cl100k_base"

  # --- Google Gemini Provider ---
  # For Gemini models.
  [providers.gemini]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__GEMINI__API_KEY or GOOGLE_API_KEY
  # api_key = "..."

  # Default model for this provider instance.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60

  # Optional: Configure Google AI's safety filters.
  # This table is passed directly to the `google-genai` SDK.
  # [providers.gemini.safety_settings]
  # HARM_CATEGORY_HARASSMENT = "BLOCK_NONE"
  # HARM_CATEGORY_HATE_SPEECH = "BLOCK_NONE"


# ==============================================================================
# [storage] - Persistence Configurations
# ==============================================================================
# This section governs where and how persistent data, such as conversation
# history (session memory) and RAG documents (semantic memory), is stored.
[storage]

  # --- Session & Episodic Memory Storage ---
  # Configures the backend for storing conversation history and agent episodes.
  [storage.session]
  # Type: Determines the storage backend.
  #   - "json": Simple, human-readable. Good for low-concurrency apps.
  #   - "sqlite": (Default) Excellent for single-process apps and local dev.
  #   - "postgres": Recommended for production, especially with multiple workers.
  # Environment Variable: LLMCORE_STORAGE__SESSION__TYPE
  type = "sqlite"

  # Path: Filesystem path for "json" and "sqlite" backends.
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_STORAGE__SESSION__PATH
  path = "~/.llmcore/sessions.db"

  # Database URL: Full connection URL for "postgres".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__SESSION__DB_URL
  # Example: db_url = "postgresql://user:password@host:port/database_name"
  db_url = ""

  # --- Vector Storage for RAG (Semantic Memory) ---
  # Configures the vector store for RAG documents and their embeddings.
  [storage.vector]
  # Type: Determines the vector database backend.
  #   - "chromadb": (Default) Easy to set up for local development.
  #   - "pgvector": Excellent for production if you already use PostgreSQL.
  #   - "pgvector_legacy": Uses original PgVectorStorage
  # Environment Variable: LLMCORE_STORAGE__VECTOR__TYPE
  type = "chromadb"

  # Default Collection: A namespace for RAG documents. Allows you to maintain
  # multiple distinct knowledge bases in the same vector store.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DEFAULT_COLLECTION
  default_collection = "llmcore_default_rag"

  # Path: Filesystem path for file-based vector stores like "chromadb".
  # Environment Variable: LLMCORE_STORAGE__VECTOR__PATH
  path = "~/.llmcore/chroma_db"

  # Database URL: Connection URL for "pgvector".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DB_URL
  db_url = ""

  # ==============================================================================
  # [storage.observability] - Storage Observability (Phase 4 - PANOPTICON)
  # ==============================================================================
  # Configures the observability layer for storage operations.
  # Provides instrumentation, metrics, event logging, and diagnostics.
  #
  # Features:
  # - Operation timing and slow query detection
  # - Prometheus-compatible metrics (counters, histograms, gauges)
  # - Persistent event logging for audit trails
  # - Distributed tracing (OpenTelemetry integration)
  #
  # Reference: Storage_System_Spec_v2r0.md Phase 4 (PANOPTICON)
  [storage.observability]

  # Master switch for all observability features.
  # When false, all observability components are disabled with zero overhead.
  # Default Value: true
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__ENABLED
  enabled = true

  # --- Query Logging ---

  # Log ALL queries at DEBUG level.
  # WARNING: Very verbose. Use only for debugging specific issues.
  # Default Value: false
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__LOG_QUERIES
  log_queries = false

  # Log queries that exceed the slow query threshold at WARNING level.
  # Recommended to keep enabled for production monitoring.
  # Default Value: true
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__LOG_SLOW_QUERIES
  log_slow_queries = true

  # Threshold (in seconds) for slow query detection.
  # Queries taking longer than this will be logged and recorded.
  # Default Value: 1.0
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__SLOW_QUERY_THRESHOLD_SECONDS
  slow_query_threshold_seconds = 1.0

  # Include query parameters in logs.
  # WARNING: Security risk - may expose sensitive data in logs.
  # Default Value: false
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__INCLUDE_QUERY_PARAMS
  include_query_params = false

  # --- Metrics Collection ---

  # Enable metrics collection (counters, histograms, gauges).
  # Default Value: true
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__METRICS_ENABLED
  metrics_enabled = true

  # Metrics backend type.
  # Possible Values: "prometheus", "memory", "none"
  #   - "prometheus": Expose metrics via HTTP endpoint for Prometheus scraping
  #   - "memory": In-memory storage for testing/debugging
  #   - "none": Disable metrics collection
  # Default Value: "memory"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__METRICS_BACKEND
  metrics_backend = "memory"

  # Prefix for all metric names.
  # Example: "llmcore_storage_operations_total" with prefix "llmcore_storage"
  # Default Value: "llmcore_storage"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__METRICS_PREFIX
  metrics_prefix = "llmcore_storage"

  # Port for Prometheus metrics HTTP endpoint (when metrics_backend = "prometheus").
  # Default Value: 9090
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__METRICS_PORT
  metrics_port = 9090

  # --- Event Logging (Persistent Audit Trail) ---

  # Enable persistent event logging to database.
  # Events are stored in the storage_events table for audit and analysis.
  # Default Value: true
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__EVENT_LOGGING_ENABLED
  event_logging_enabled = true

  # Days to retain events in the database.
  # Set to 0 to retain events forever (not recommended for production).
  # Default Value: 30
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__EVENT_RETENTION_DAYS
  event_retention_days = 30

  # Database table name for event storage.
  # Default Value: "storage_events"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__EVENT_TABLE_NAME
  event_table_name = "storage_events"

  # Number of events to batch before flushing to database.
  # Higher values reduce database writes but increase memory usage.
  # Default Value: 100
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__EVENT_BATCH_SIZE
  event_batch_size = 100

  # Interval (in seconds) for flushing events to database.
  # Events are flushed when batch_size is reached OR this interval elapses.
  # Default Value: 5.0
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__EVENT_FLUSH_INTERVAL_SECONDS
  event_flush_interval_seconds = 5.0

  # --- Distributed Tracing (OpenTelemetry) ---

  # Enable distributed tracing.
  # Requires OpenTelemetry SDK to be installed and configured.
  # Default Value: false
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__TRACING_ENABLED
  tracing_enabled = false

  # Tracing backend type.
  # Possible Values: "opentelemetry", "none"
  # Default Value: "none"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__TRACING_BACKEND
  tracing_backend = "none"

  # OTLP collector endpoint for trace export.
  # Default Value: "http://localhost:4317"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__TRACING_ENDPOINT
  tracing_endpoint = "http://localhost:4317"

  # Service name for traces.
  # Default Value: "llmcore-storage"
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__TRACING_SERVICE_NAME
  tracing_service_name = "llmcore-storage"

  # Sampling rate for traces (0.0 to 1.0).
  # 1.0 = trace all requests, 0.1 = trace 10% of requests.
  # Default Value: 1.0
  # Environment Variable: LLMCORE_STORAGE__OBSERVABILITY__TRACING_SAMPLE_RATE
  tracing_sample_rate = 1.0

  # ==============================================================================
  # [model_cards] - Model Card Library Configuration
  # ==============================================================================
  # The Model Card Library provides comprehensive metadata for LLM models,
  # including context limits, capabilities, pricing, and lifecycle information.
  # Cards are loaded from built-in defaults and can be overridden by user cards.
  [model_cards]

  # Path to user-defined model cards directory.
  # Cards in this directory override built-in cards with the same model_id.
  # Organize cards by provider subdirectory: model_cards/<provider>/<model>.json
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_MODEL_CARDS__USER_CARDS_PATH
  user_cards_path = "~/.config/llmcore/model_cards"

  # Automatically load model cards on first registry access.
  # Set to false if you want to control loading timing manually via registry.load()
  # Environment Variable: LLMCORE_MODEL_CARDS__AUTO_LOAD
  auto_load = true

  # Validation strictness for model card files.
  # If true, invalid JSON files will raise errors during loading.
  # If false, invalid files are logged as warnings and skipped.
  # Environment Variable: LLMCORE_MODEL_CARDS__STRICT_VALIDATION
  strict_validation = false

# ==============================================================================
# [embedding] - Vectorization Model Configurations
# ==============================================================================
# This section configures the models responsible for converting text into
# numerical vectors (embeddings) for the RAG system. The global
# `llmcore.default_embedding_model` setting determines which model to use.
# If that identifier contains a colon (e.g., "openai:text-embedding-3-small"),
# the part before the colon tells the system which configuration block below
# to consult for specific settings like API keys.
[embedding]

  [embedding.openai]
  # API Key for OpenAI embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__OPENAI__API_KEY
  # api_key = ""
  default_model = "text-embedding-3-small"
  # base_url = "..." # Optional proxy URL

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__GOOGLE__API_KEY
  # api_key = ""
  default_model = "models/embedding-001"

  [embedding.ollama]
  # Host URL for the Ollama server for embeddings.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings. Ensure it's pulled locally.
  # Examples: "mxbai-embed-large", "nomic-embed-text"
  default_model = "mxbai-embed-large"
  timeout = 60

  [embedding.sentence_transformer]
  # Additional configuration for local `sentence-transformers` models.
  # The model itself is specified in `llmcore.default_embedding_model`.
  # Device to run the model on. If not set, the library will auto-detect.
  # Options: "cpu", "cuda", "mps"
  # device = "cpu"


  # =============================================================================
  # [embedding.cache] - Embedding Cache Configuration
  # =============================================================================
  # Caches embedding vectors to reduce API costs and improve performance.
  # Uses a two-tier architecture: LRU in-memory cache + SQLite disk cache.
  #
  # Reference: UNIFIED_IMPLEMENTATION_PLAN.md Phase 1, Task 1.2

  [embedding.cache]
  # --- Enable/Disable Caching ---
  # What it does: Master switch for embedding caching.
  # Purpose: Reduce API costs by caching identical embeddings.
  # Default Value: true
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__ENABLED
  enabled = true

  # --- Memory Cache Size ---
  # What it does: Maximum number of embeddings to keep in memory (LRU cache).
  # Purpose: Fast access to frequently used embeddings.
  # Default Value: 10000
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__MEMORY_SIZE
  memory_size = 10000

  # --- Disk Cache Enable ---
  # What it does: Enable SQLite disk persistence for embeddings.
  # Purpose: Cache survives process restarts.
  # Default Value: true
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__DISK_ENABLED
  disk_enabled = true

  # --- Disk Cache Path ---
  # What it does: Path to SQLite database for cached embeddings.
  # Purpose: Persistent storage location.
  # Default Value: "~/.cache/llmcore/embeddings.db"
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__DISK_PATH
  disk_path = "~/.cache/llmcore/embeddings.db"

  # --- Disk Cache Max Entries ---
  # What it does: Maximum number of embeddings to store on disk (0 = unlimited).
  # Purpose: Limit disk space usage.
  # Default Value: 0 (unlimited)
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__DISK_MAX_ENTRIES
  disk_max_entries = 0

  # --- Cache TTL (Time-to-Live) ---
  # What it does: Hours before cached embeddings expire (0 = never expire).
  # Purpose: Ensure cache freshness when embedding models change.
  # Default Value: 0 (no expiration)
  # Environment Variable: LLMCORE_EMBEDDING__CACHE__TTL_HOURS
  ttl_hours = 0


  # =============================================================================
  # [observability] - General Observability Configuration
  # =============================================================================
  # Observability features for tracking API usage, costs, and performance.
  #
  # Reference: UNIFIED_IMPLEMENTATION_PLAN.md Phase 1, Task 1.4

  [observability]
  # Top-level observability enable (affects all observability features)
  enabled = true

    # =========================================================================
    # [observability.cost_tracking] - Cost Tracking Configuration
    # =========================================================================
    # Tracks token usage and estimated costs for all LLM and embedding API calls.

    [observability.cost_tracking]
    # --- Enable Cost Tracking ---
    # What it does: Enable recording of API usage and cost estimation.
    # Purpose: Budget monitoring and usage analytics.
    # Default Value: true
    # Environment Variable: LLMCORE_OBSERVABILITY__COST_TRACKING__ENABLED
    enabled = true

    # --- Cost Database Path ---
    # What it does: Path to SQLite database for cost records.
    # Purpose: Persistent storage for usage analytics.
    # Default Value: "~/.llmcore/costs.db"
    # Environment Variable: LLMCORE_OBSERVABILITY__COST_TRACKING__DB_PATH
    db_path = "~/.llmcore/costs.db"

    # --- Retention Period ---
    # What it does: Days to retain cost records (0 = forever).
    # Purpose: Manage database size.
    # Default Value: 90
    # Environment Variable: LLMCORE_OBSERVABILITY__COST_TRACKING__RETENTION_DAYS
    retention_days = 90

    # --- Console Logging ---
    # What it does: Log usage records to console.
    # Purpose: Real-time visibility during development.
    # Default Value: false
    # Environment Variable: LLMCORE_OBSERVABILITY__COST_TRACKING__LOG_TO_CONSOLE
    log_to_console = false

    # --- Track Latency ---
    # What it does: Record request latency with usage records.
    # Purpose: Performance monitoring.
    # Default Value: true
    # Environment Variable: LLMCORE_OBSERVABILITY__COST_TRACKING__TRACK_LATENCY
    track_latency = true


  # =============================================================================
  # [agents.observability] - Agent Observability & Telemetry
  # =============================================================================
  # This section configures the observability system for agent executions.
  # The observability module provides:
  #   - Structured event logging (JSONL format)
  #   - Execution metrics collection
  #   - Execution replay for debugging
  #
  # Events are organized by category:
  #   - LIFECYCLE: Agent start/stop events
  #   - COGNITIVE: Phase execution (THINK, ACT, OBSERVE, REFLECT)
  #   - ACTIVITY: Tool/activity executions
  #   - HITL: Human-in-the-loop approvals
  #   - ERROR: Errors and exceptions
  #   - METRIC: Performance metrics
  #   - MEMORY: Memory operations
  #   - SANDBOX: Container/VM lifecycle
  #   - RAG: Retrieval operations
  #
  # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Sections 26-28

  [agents.observability]

  # --- Master Enable/Disable ---
  # What it does: Master switch for the entire observability system.
  # Purpose: Quickly disable all observability features without changing other settings.
  # Possible Options: true, false
  # Default Value: true
  # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__ENABLED
  enabled = true

  # =============================================================================
  # Event Logging Settings
  # =============================================================================

      [agents.observability.events]
      # --- Event Logging Enable ---
      # What it does: Enable/disable structured event logging to file.
      # Purpose: Capture detailed execution traces for debugging and analysis.
      # Possible Options: true, false
      # Default Value: true
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__EVENTS__ENABLED
      enabled = true

      # --- Event Log File Path ---
      # What it does: Path to the JSONL file where events are written.
      # Purpose: Persistent storage for execution events.
      # Format: Events are written as newline-delimited JSON (JSONL/NDJSON).
      # Default Value: "~/.llmcore/events.jsonl"
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__EVENTS__LOG_PATH
      log_path = "~/.llmcore/events.jsonl"

      # --- Minimum Event Severity ---
      # What it does: Filter events by minimum severity level.
      # Purpose: Control verbosity of event logging.
      # Possible Options: "debug", "info", "warning", "error", "critical"
      # Default Value: "info"
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__EVENTS__MIN_SEVERITY
      min_severity = "info"

      # --- Event Categories to Log ---
      # What it does: Whitelist of event categories to capture.
      # Purpose: Selectively enable/disable logging for specific event types.
      # Possible Values: "lifecycle", "cognitive", "activity", "hitl", "error",
      #                  "metric", "memory", "sandbox", "rag"
      # Default Value: (all categories)
      # Note: Empty list means log ALL categories.
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__EVENTS__CATEGORIES
      categories = []

      # --- Log Rotation Settings ---
      # What it does: Configure automatic log file rotation.
      # Purpose: Prevent unbounded log file growth.

          [agents.observability.events.rotation]
          # Rotation strategy: "none", "daily", "size", "both"
          # - none: No rotation, single file grows indefinitely
          # - daily: Rotate at midnight each day
          # - size: Rotate when file exceeds max_size_mb
          # - both: Rotate on either condition
          # Default Value: "size"
          strategy = "size"

          # Maximum log file size in MB before rotation (if strategy includes "size").
          # Default Value: 100
          max_size_mb = 100

          # Number of rotated log files to keep. 0 = keep all.
          # Default Value: 10
          max_files = 10

          # Compress rotated files with gzip.
          # Default Value: true
          compress = true

  # =============================================================================
  # Buffering Settings
  # =============================================================================

      [agents.observability.buffer]
      # --- Buffering Enable ---
      # What it does: Enable write buffering for better performance.
      # Purpose: Batch multiple events before writing to reduce I/O overhead.
      # Possible Options: true, false
      # Default Value: true
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__BUFFER__ENABLED
      enabled = true

      # --- Buffer Size ---
      # What it does: Maximum number of events to buffer before flushing.
      # Purpose: Balance between I/O efficiency and data freshness.
      # Default Value: 100
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__BUFFER__SIZE
      size = 100

      # --- Flush Interval ---
      # What it does: Maximum time (seconds) to hold events before flushing.
      # Purpose: Ensure events are persisted even during low activity.
      # Default Value: 5
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__BUFFER__FLUSH_INTERVAL_SECONDS
      flush_interval_seconds = 5

      # --- Flush on Shutdown ---
      # What it does: Flush all buffered events when agent shuts down.
      # Purpose: Prevent data loss on graceful shutdown.
      # Possible Options: true, false
      # Default Value: true
      flush_on_shutdown = true

  # =============================================================================
  # Metrics Collection Settings
  # =============================================================================

      [agents.observability.metrics]
      # --- Metrics Collection Enable ---
      # What it does: Enable/disable execution metrics collection.
      # Purpose: Track performance, cost, and usage statistics.
      # Possible Options: true, false
      # Default Value: true
      # Environment Variable: LLMCORE_AGENTS__OBSERVABILITY__METRICS__ENABLED
      enabled = true

      # --- Metrics to Collect ---
      # What it does: Whitelist of metric types to collect.
      # Purpose: Control which metrics are tracked.
      # Possible Values:
      #   - "iterations": Cognitive loop iterations
      #   - "llm_calls": LLM API calls (latency, tokens, cost)
      #   - "activities": Tool/activity executions
      #   - "hitl": Human approval metrics
      #   - "errors": Error counts and types
      #   - "duration": Execution timing
      #   - "tokens": Token usage
      #   - "cost": Estimated costs
      # Default Value: (all metrics)
      # Note: Empty list means collect ALL metrics.
      collect = []

      # --- Cost Tracking ---
      # What it does: Enable estimated cost calculation for LLM calls.
      # Purpose: Budget monitoring and optimization.
      # Possible Options: true, false
      # Default Value: true
      track_cost = true

      # --- Token Tracking ---
      # What it does: Track input/output token counts per call.
      # Purpose: Usage analysis and optimization.
      # Possible Options: true, false
      # Default Value: true
      track_tokens = true

      # --- Latency Percentiles ---
      # What it does: Percentiles to calculate for latency metrics.
      # Purpose: Understand latency distribution (p50, p95, p99).
      # Default Value: [50, 90, 95, 99]
      latency_percentiles = [50, 90, 95, 99]

  # =============================================================================
  # Replay Settings
  # =============================================================================

      [agents.observability.replay]
      # --- Replay Enable ---
      # What it does: Enable execution replay functionality.
      # Purpose: Debug and analyze past executions step-by-step.
      # Possible Options: true, false
      # Default Value: true
      # Note: Requires events.enabled = true
      enabled = true

      # --- Replay Cache ---
      # What it does: Cache parsed events in memory for faster replay.
      # Purpose: Speed up repeated replay operations.
      # Possible Options: true, false
      # Default Value: true
      cache_enabled = true

      # --- Replay Cache Size ---
      # What it does: Maximum number of executions to cache.
      # Purpose: Limit memory usage.
      # Default Value: 50
      cache_max_executions = 50

  # =============================================================================
  # Sinks Configuration
  # =============================================================================
  # Sinks define where events are written. Multiple sinks can be active.

      [agents.observability.sinks]
      # --- File Sink ---
      # Primary sink - writes to JSONL file (configured above in events.log_path)
      file_enabled = true

      # --- Memory Sink (Debug) ---
      # What it does: Store events in memory for debugging/testing.
      # Purpose: Inspect events without file I/O during development.
      # WARNING: Memory usage grows with event count. Use only for debugging.
      # Possible Options: true, false
      # Default Value: false
      memory_enabled = false

      # Maximum events to keep in memory sink. 0 = unlimited.
      # Default Value: 1000
      memory_max_events = 1000

      # --- Callback Sink ---
      # What it does: Call a user-defined function for each event.
      # Purpose: Custom integrations (e.g., send to external monitoring).
      # Note: Callback must be registered programmatically, this just enables it.
      # Possible Options: true, false
      # Default Value: false
      callback_enabled = false

  # =============================================================================
  # Performance Settings
  # =============================================================================

      [agents.observability.performance]
      # --- Async Logging ---
      # What it does: Write events asynchronously to avoid blocking execution.
      # Purpose: Minimize observability overhead on agent performance.
      # Possible Options: true, false
      # Default Value: true
      async_logging = true

      # --- Sampling Rate ---
      # What it does: Sample rate for high-frequency events (0.0 to 1.0).
      # Purpose: Reduce volume for very active agents.
      # Example: 0.1 = log 10% of events, 1.0 = log all events
      # Default Value: 1.0 (log everything)
      # Note: Lifecycle and error events are never sampled.
      sampling_rate = 1.0

      # --- Max Event Size ---
      # What it does: Maximum size (bytes) for event data fields.
      # Purpose: Prevent huge events from bloating log files.
      # Default Value: 10000 (10KB)
      # Note: Large data is truncated with "[TRUNCATED]" marker.
      max_event_data_bytes = 10000

      # --- Overhead Warning Threshold ---
      # What it does: Warn if logging overhead exceeds this percentage of execution time.
      # Purpose: Detect performance problems from observability.
      # Default Value: 5 (5%)
      overhead_warning_threshold_percent = 5


# ==============================================================================
# [context_management] - Advanced Prompt Control
# ==============================================================================
# This section is the control center for prompt engineering. It governs the
# sophisticated, multi-stage process of assembling the final context that is
# sent to the LLM. The system first prioritizes what to INCLUDE, and only then
# decides what to TRUNCATE if the model's token limit is exceeded.
[context_management]

  # --- Inclusion Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               components are added to the prompt.
  # Purpose: Gives fine-grained control over the final prompt's structure.
  # Valid Components: "system_history", "explicitly_staged", "user_items_active",
  #                   "history_chat", "final_user_query".
  # Default: "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"
  inclusion_priority = "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"

  # --- Truncation Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               types are removed to meet token limits.
  # Purpose: Controls what information is sacrificed when the context is too long.
  # Valid Components: "history_chat", "user_items_active", "rag_in_query",
  #                   "explicitly_staged".
  # Default: "history_chat,user_items_active,rag_in_query,explicitly_staged"
  truncation_priority = "history_chat,user_items_active,rag_in_query,explicitly_staged"

  # --- RAG Prompt Template ---
  # What it does: The template used to format the final query when RAG is enabled.
  #               It will be formatted with `{context}` (retrieved documents) and
  #               `{question}` (the user's query).
  # Purpose: To instruct the LLM on how to use the provided RAG context.
  default_prompt_template = """You are an AI assistant specialized in answering questions about codebases
  based on provided context.
  Use ONLY the following pieces of retrieved context to answer the user's question.
  If the answer is not found in the context, state that you cannot answer based on the provided information.
  Do not make up an answer or use external knowledge. Keep the answer concise and relevant to the question.
  Include relevant source file paths and line numbers if possible, based *only* on the provided context metadata.

  Context:
  ---------------------
  {context}
  ---------------------

  Question: {question}

  Answer:"""

    # Optional path to a custom prompt template file. Overrides the string above.
    prompt_template_path = ""

    # Default number of documents to retrieve for RAG.
    rag_retrieval_k = 3

    # Number of tokens to leave free in the context window for the model's response.
    reserved_response_tokens = 500

    # Minimum number of recent chat messages to try to keep during truncation.
    minimum_history_messages = 1

    # Prioritizes keeping the N most recent user messages (and their preceding
    # assistant responses) during truncation.
    user_retained_messages_count = 5

    # A safeguard character limit for a single user-provided context item.
    # The default is very large to accommodate large documents.
    max_chars_per_user_item = 40000000

    # ==============================================================================
    # Step 2.1: Semantiscan Configuration Section (CORRECTED & COMPLETE)
    # ==============================================================================
    # Purpose: Unified configuration for the semantiscan RAG/ingestion engine.
    # After Phase 2, semantiscan no longer maintains its own config.toml;
    # instead, it receives configuration from llmcore via this section.
    #
    # References:
    # - semantiscan/config/loaders.py (DEFAULT_CONFIG)
    # - semantiscan/config/models.py (AppConfig schema)
    # - Resurrection Plan Section VI, Task 1.3 & Phase 2, Step 2.1
    # ==============================================================================

    [semantiscan]
    # Top-level semantiscan enablement flag (for future use)
    enabled = true

        # --- Database Configuration (Vector Store) ---
        # Defines where semantiscan stores embedded code chunks for RAG retrieval.
        # IMPORTANT: For unified storage, this should match llmcore's [storage.vector.path]
        [semantiscan.database]
        type = "chromadb"  # Currently only ChromaDB is supported by semantiscan
        collection_name = "default_semantiscan"  # Default collection for ingested code
        vector_backend = "llmcore"  # or "chromadb" (default)
        user_id = "project_123"     # optional multi-tenant isolation
        namespace = "dev"           # optional collection prefix
        enable_hybrid_search = true # optional pgvector hybrid search
        # type = "chromadb"  # Currently only ChromaDB is supported by semantiscan
        # # Default path - should be synchronized with llmcore's vector storage path
        path = "~/.llmcore/chroma_db"
        # collection_name = "default_semantiscan"  # Default collection for ingested code


        # --- Metadata Store Configuration ---
        # Optional SQLite database for tracking ingestion history, Git metadata, etc.
        # CORRECTED: Field names now match semantiscan.config.models.MetadataStoreConfig
        [semantiscan.metadata_store]
        enable = false  # Disabled by default; enable for Git-aware ingestion tracking
        type = "sqlite"  # Type of external store (sqlite or postgresql - postgresql future)
        path = "~/.local/share/semantiscan/metadata.db"
        connection_string = ""  # Connection string for PostgreSQL (required if type is postgresql)
        # Primary table for storing rich chunk metadata
        table_name = "chunk_metadata"
        # Table for tracking ingestion state per repo/branch
        ingestion_log_table_name = "ingestion_log"
        # Table for tracking file changes per commit ('historical_delta' mode)
        file_history_table_name = "file_history"

        # --- Embedding Configuration ---
        # Defines which embedding models are available for chunking ingestion.
        # semantiscan will use llmcore's EmbeddingManager for actual embedding generation.
        [semantiscan.embeddings]
        default_model = "sentence_transformer_local"  # Default embedding model key

        # Define available embedding models (keys reference llmcore's [embedding.*] sections)
        [semantiscan.embeddings.models.sentence_transformer_local]
        provider = "sentence-transformers"  # Maps to llmcore's sentence_transformer provider
        model_name = "all-MiniLM-L6-v2"  # Default local model
        device = "cpu"  # "cpu" | "cuda" | "mps" (for M1/M2 Macs)
        max_request_tokens = 8000  # Max tokens per embedding batch request
        base_url = ""  # Not used for sentence-transformers
        tokenizer_name = ""  # Optional: Specific tokenizer name
        uses_doc_query_prefixes = false  # Set to true if model requires different prefixes
        query_prefix = ""  # Prefix for queries if uses_doc_query_prefixes is true
        document_prefix = ""  # Prefix for documents if uses_doc_query_prefixes is true

        [semantiscan.embeddings.models.openai_ada]
        provider = "openai"  # Maps to llmcore's openai embedding provider
        model_name = "text-embedding-ada-002"
        device = "cpu"  # Not used for API-based models
        api_key_env = "OPENAI_API_KEY"  # Reference to environment variable
        max_request_tokens = 8000
        base_url = ""  # Optional: Custom API endpoint
        tokenizer_name = "cl100k_base"  # OpenAI tokenizer
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        [semantiscan.embeddings.models.openai_large]
        provider = "openai"
        model_name = "text-embedding-3-large"
        device = "cpu"
        api_key_env = "OPENAI_API_KEY"
        max_request_tokens = 8000
        base_url = ""
        tokenizer_name = "cl100k_base"
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        [semantiscan.embeddings.models.ollama_nomic]
        provider = "ollama"
        model_name = "nomic-embed-text:latest"
        device = "cpu"  # Not used for Ollama
        api_key_env = ""  # Not needed for local Ollama
        max_request_tokens = 2048
        base_url = "http://localhost:11434"
        tokenizer_name = ""  # Optional
        uses_doc_query_prefixes = false
        query_prefix = ""
        document_prefix = ""

        # --- Chunking Configuration ---
        # Defines how different file types are parsed and chunked during ingestion.
        [semantiscan.chunking]
        default_strategy = "RecursiveSplitter"  # Fallback for unknown file types

        # ANTLR-based code parsers (requires ANTLR runtime and generated parsers)
        [semantiscan.chunking.strategies.python_code]
        extensions = [".py", ".pyw"]
        grammar = "Python3"
        entry_points = ["funcdef", "async_funcdef", "classdef", "decorated"]
        parser = ""  # Not used for ANTLR strategy
        method = ""  # Not used for ANTLR strategy
        strategy_sequence = []  # Not used for simple strategy
        hybrid_content = false  # Combine related elements in ANTLR chunks

        [semantiscan.chunking.strategies.java_code]
        extensions = [".java"]
        grammar = "Java"
        entry_points = ["methodDeclaration", "classDeclaration", "interfaceDeclaration"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.cpp_code]
        extensions = [".cpp", ".cc", ".cxx", ".hpp", ".h", ".hxx"]
        grammar = "CPP14"
        entry_points = ["functionDefinition", "classSpecifier"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.go_code]
        extensions = [".go"]
        grammar = "Golang"
        entry_points = ["functionDecl", "methodDecl", "typeSpec"]
        parser = ""
        method = ""
        strategy_sequence = []
        hybrid_content = false

        # Format-specific parsers (YAML, JSON, TOML, XML, etc.)
        [semantiscan.chunking.strategies.yaml_format]
        extensions = [".yaml", ".yml"]
        grammar = ""  # Not used for format strategy
        entry_points = []
        parser = "PyYAML"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.json_format]
        extensions = [".json"]
        grammar = ""
        entry_points = []
        parser = "json"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.toml_format]
        extensions = [".toml"]
        grammar = ""
        entry_points = []
        parser = "tomli"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.xml_format]
        extensions = [".xml"]
        grammar = ""
        entry_points = []
        parser = "xml.etree.ElementTree"
        method = ""
        strategy_sequence = []
        hybrid_content = false

        # Agnostic text splitters (for markdown, plain text, logs, etc.)
        [semantiscan.chunking.strategies.markdown_agnostic]
        extensions = [".md", ".markdown"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.text_agnostic]
        extensions = [".txt"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.logs_agnostic]
        extensions = [".log"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "LineSplitter"
        strategy_sequence = []
        hybrid_content = false

        [semantiscan.chunking.strategies.rst_agnostic]
        extensions = [".rst"]
        grammar = ""
        entry_points = []
        parser = ""
        method = "RecursiveSplitter"
        strategy_sequence = []
        hybrid_content = false

        # Parameters for agnostic chunking methods
        [semantiscan.chunking.parameters.RecursiveSplitter]
        chunk_size = 1000  # Target chunk size in characters
        chunk_overlap = 150  # Overlap between chunks to preserve context

        [semantiscan.chunking.parameters.LineSplitter]
        lines_per_chunk = 50  # Number of lines per chunk

        [semantiscan.chunking.parameters.SubChunker]
        chunk_size = 500  # Chunk size for sub-chunking oversized chunks
        chunk_overlap = 50  # Overlap for sub-chunking

        # --- Ingestion Configuration ---
        # Controls the ingestion pipeline behavior
        [semantiscan.ingestion]
        embedding_workers = 4  # Number of parallel workers for embedding generation
        batch_size = 100  # Number of chunks to process in parallel batches

        # Git-aware ingestion modes (advanced feature)
        [semantiscan.ingestion.git]
        enabled = false  # Enable Git-aware ingestion tracking
        default_ref = "main"  # Default branch to ingest
        ingestion_mode = "snapshot"  # Modes: "snapshot" | "historical" | "historical_delta" | "incremental"
        historical_start_ref = ""  # Starting ref for historical ingestion modes
        enable_commit_analysis = false  # Extract commit messages, authors, etc.
        enable_commit_llm_analysis = false  # Use LLM to analyze/summarize commit messages
        commit_llm_provider_key = ""  # LLM provider key for commit analysis
        commit_llm_prompt_template = ""  # Custom prompt template for commit analysis
        commit_message_filter_regex = []  # Regex patterns to filter out certain commit messages

        # --- LLM Configuration (for query answering) ---
        # NOTE: In Phase 2, semantiscan will use llmcore's LLM providers directly.
        # This section is for query answering and advanced features like query rewriting.
        [semantiscan.llm]
        default_provider = "ollama_llama3"  # Key referencing llmcore's provider
        prompt_template_path = ""  # Optional: Custom prompt template file path
        enable_query_rewriting = false  # Enable LLM-based query expansion
        query_rewrite_provider_key = ""  # LLM provider for query rewriting
        show_sources_in_text = true  # Include source references in text responses
        tokenizer_name = ""  # Optional: Specific tokenizer for LLM context estimation
        context_buffer = 200  # Reserved tokens for prompt formatting

        # Reference to llmcore providers (will be resolved at runtime)
        [semantiscan.llm.providers.ollama_llama3]
        provider = "ollama"  # Maps to llmcore's ollama provider
        model_name = "llama3:8b"
        base_url = "http://localhost:11434"
        api_key_env = ""  # Not needed for Ollama
        tokenizer_name = ""
        context_buffer = 250
        # Parameters passed directly to the LLM API
        [semantiscan.llm.providers.ollama_llama3.parameters]
        temperature = 0.5
        num_ctx = 4096
        top_p = 0.9

        [semantiscan.llm.providers.openai_gpt4]
        provider = "openai"
        model_name = "gpt-4"
        base_url = ""  # Optional: Custom endpoint
        api_key_env = "OPENAI_API_KEY"
        tokenizer_name = "cl100k_base"
        context_buffer = 200
        [semantiscan.llm.providers.openai_gpt4.parameters]
        temperature = 0.2
        max_tokens = 4000

        [semantiscan.llm.providers.openai_gpt4_turbo]
        provider = "openai"
        model_name = "gpt-4-turbo-preview"
        base_url = ""
        api_key_env = "OPENAI_API_KEY"
        tokenizer_name = "cl100k_base"
        context_buffer = 200
        [semantiscan.llm.providers.openai_gpt4_turbo.parameters]
        temperature = 0.2
        max_tokens = 4000

        # --- Retrieval Configuration ---
        # Controls how semantiscan retrieves relevant chunks during RAG queries.
        [semantiscan.retrieval]
        top_k = 10  # Number of most relevant chunks to retrieve
        enable_hybrid_search = false  # Use hybrid search (semantic + keyword BM25)
        bm25_k1 = 1.5  # BM25 parameter: term frequency saturation
        bm25_b = 0.75  # BM25 parameter: length normalization
        enrich_with_external_metadata = false  # Include file metadata from external store in retrieval

        # --- File Discovery Configuration ---
        # Controls which files are included/excluded during ingestion.
        [semantiscan.discovery]
        use_gitignore = true  # Respect .gitignore rules during file scanning
        excluded_dirs = [  # Directories to always skip (even if not in .gitignore)
            "__pycache__",
            "node_modules",
            ".git",
            "venv",
            ".venv",
            "build",
            "dist",
            ".pytest_cache",
            ".mypy_cache",
            "htmlcov",
            ".tox",
            ".eggs",
            "*.egg-info",
            "target"  # Rust/Java build directories
        ]
        excluded_files = [".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.pyd"]  # Files to always skip

        # --- Logging Configuration ---
        # Semantiscan-specific logging (separate from llmcore's logging)
        [semantiscan.logging]
        log_level_console = "INFO"  # Console log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
        log_file_enabled = false  # Enable file logging for semantiscan
        log_directory = "~/.local/share/semantiscan/logs"  # Directory for log files
        log_filename_template = "semantiscan_{timestamp:%Y%m%d_%H%M%S}.log"  # Log filename template
        log_level_file = "DEBUG"  # File log level (more verbose than console)
        log_format = "%(asctime)s [%(levelname)-8s] %(name)-30s - %(message)s (%(filename)s:%(lineno)d)"

        # ==============================================================================
        # SANDBOX ADDITIONS FOR default_config.toml
        # ==============================================================================
        # INSTRUCTIONS: Append this content to the END of your existing default_config.toml
        # Do NOT replace the existing file - just add these sections at the bottom.
        # ==============================================================================

        # ==============================================================================
        # [agents] - Agent System Configuration
        # ==============================================================================
        [agents]

        # --- Agent Execution Settings ---
        # Default maximum iterations for agent loops
        max_iterations = 10

        # Default timeout for agent tasks (in seconds)
        default_timeout = 600

        # ==============================================================================
        # [agents.sandbox] - Sandbox Configuration for Agent Execution
        # ==============================================================================
        # This section configures the sandbox system for isolated agent code execution.
        # When enabled, agent tasks execute in Docker containers or VMs, ensuring
        # that agent-generated code NEVER runs on the host system.
        #
        # SECURITY: The sandbox is designed to prevent agents from accessing or
        # modifying the host system. All code execution happens in isolated environments.

        [agents.sandbox]
        # Sandbox mode determines which backend to use for isolation.
        # Possible Values: "docker", "vm", "hybrid"
        #   - docker: Use Docker containers (recommended for most use cases)
        #   - vm: Use SSH to a dedicated VM (for high-security scenarios)
        #   - hybrid: Try Docker first, fall back to VM if Docker unavailable
        # Default Value: "docker"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__MODE
        mode = "docker"

        # Enable fallback to VM if primary mode fails (only applies to hybrid mode)
        # Default Value: true
        fallback_enabled = true

        # --- Docker Sandbox Settings ---
        [agents.sandbox.docker]
        # Enable Docker sandbox support
        # Default Value: true
        enabled = true

        # Default Docker image for sandbox containers.
        # Should be a Python image with necessary tools.
        # Default Value: "python:3.11-slim"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__IMAGE
        image = "python:3.11-slim"

        # Whitelist of allowed Docker images (glob patterns supported).
        # Only images matching these patterns can be used.
        # Default Value: ["python:3.*-slim", "python:3.*-bookworm"]
        image_whitelist = [
            "python:3.*-slim",
            "python:3.*-bookworm",
            "llmcore-sandbox:*"
        ]

        # Docker label that grants FULL access level.
        # Containers with this label bypass tool restrictions.
        # Default Value: "llmcore.sandbox.full_access=true"
        full_access_label = "llmcore.sandbox.full_access=true"

        # Container name patterns that grant FULL access (glob patterns).
        # Default Value: []
        full_access_name_patterns = [
            "llmcore-trusted-*"
        ]

        # Memory limit for containers (Docker memory limit format).
        # Default Value: "1g"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__MEMORY_LIMIT
        memory_limit = "1g"

        # CPU limit for containers (number of CPU cores).
        # Default Value: 2.0
        cpu_limit = 2.0

        # Default timeout for operations in seconds.
        # Default Value: 600
        timeout_seconds = 600

        # Enable network access in containers.
        # WARNING: Enabling network access reduces isolation security.
        # Default Value: false
        network_enabled = false

        # Docker host URL (for remote Docker daemons).
        # Leave empty for local Docker socket.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__DOCKER__HOST
        # docker_host = "tcp://192.168.1.100:2375"

        # --- VM Sandbox Settings ---
        [agents.sandbox.vm]
        # Enable VM sandbox support (requires SSH access to a VM).
        # Default Value: false
        enabled = false

        # VM host address.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VM__HOST
        host = ""

        # SSH port.
        # Default Value: 22
        port = 22

        # SSH username.
        # Default Value: "agent"
        username = "agent"

        # Path to SSH private key file.
        # WARNING: Use SSH agent or environment variable in production.
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VM__PRIVATE_KEY_PATH
        private_key_path = ""

        # Use SSH agent for authentication instead of key file.
        # Default Value: false
        use_ssh_agent = false

        # List of hosts that grant FULL access level.
        # VMs in this list bypass tool restrictions.
        # Default Value: []
        full_access_hosts = []

        # Default timeout for operations in seconds.
        # Default Value: 600
        timeout_seconds = 600

        # Working directory on the VM.
        # Default Value: "/tmp/llmcore_sandbox"
        working_directory = "/tmp/llmcore_sandbox"

        # --- Volume Mount Settings ---
        [agents.sandbox.volumes]
        # Host path for shared data that persists across sandbox instances.
        # This directory is mounted read-write in sandboxes.
        # Default Value: "~/.llmcore/agent_share"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VOLUMES__SHARE_PATH
        share_path = "~/.llmcore/agent_share"

        # Host path for agent output files.
        # Files saved here persist after sandbox cleanup.
        # Default Value: "~/.llmcore/agent_outputs"
        # Environment Variable: LLMCORE_AGENTS__SANDBOX__VOLUMES__OUTPUTS_PATH
        outputs_path = "~/.llmcore/agent_outputs"

        # --- Tool Access Control ---
        [agents.sandbox.tools]
        # List of tools allowed for RESTRICTED access level sandboxes.
        # FULL access sandboxes bypass this restriction.
        # Default Value: (all standard sandbox tools)
        allowed = [
            "execute_shell",
            "execute_python",
            "save_file",
            "load_file",
            "replace_in_file",
            "append_to_file",
            "list_files",
            "file_exists",
            "delete_file",
            "create_directory",
            "get_state",
            "set_state",
            "list_state",
            "get_sandbox_info",
            "get_recorded_files",
            # Core agent tools
            "semantic_search",
            "episodic_search",
            "calculator",
            "finish",
            "human_approval"
        ]

        # List of tools explicitly denied for all access levels.
        # These tools are never available, even for FULL access.
        # Default Value: []
        denied = [
            # Example dangerous tools that should never be allowed:
            # "sudo_execute",
            # "raw_socket",
            # "install_system_package"
        ]

        # --- Output Tracking Settings ---
        [agents.sandbox.output_tracking]
        # Enable persistent tracking of agent outputs.
        # Default Value: true
        enabled = true

        # Maximum number of execution log entries per run.
        # Default Value: 1000
        max_log_entries = 1000

        # Maximum age of runs to keep (in days). 0 = keep forever.
        # Default Value: 30
        max_run_age_days = 30

        # Maximum number of runs to keep. 0 = no limit.
        # Default Value: 100
        max_runs = 100

        # ==============================================================================
        # [agents.goals] - Goal Classification (G3 Phase 3)
        # ==============================================================================
        # Configures how goals are classified by complexity to enable fast-path
        # routing and appropriate iteration limits.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.goals]

        # Enable goal complexity classification
        # When enabled, goals are analyzed before execution to determine complexity.
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__GOALS__CLASSIFIER_ENABLED
        classifier_enabled = true

        # Use LLM for uncertain classifications
        # Falls back to LLM when heuristic confidence is below threshold.
        # Slower but more accurate for edge cases.
        # Default Value: false
        # Environment Variable: LLMCORE_AGENTS__GOALS__USE_LLM_FALLBACK
        use_llm_fallback = false

        # Confidence threshold for heuristic classification
        # If heuristic confidence is below this, uses LLM fallback (if enabled).
        # Default Value: 0.9
        heuristic_confidence_threshold = 0.9

        # Maximum iterations by complexity level
        # These override max_iterations based on goal complexity.
        trivial_max_iterations = 1
        simple_max_iterations = 5
        moderate_max_iterations = 10
        complex_max_iterations = 15

        # ==============================================================================
        # [agents.fast_path] - Fast Path Execution (G3 Phase 3)
        # ==============================================================================
        # Configures the fast-path executor for trivial goals.
        # Fast-path bypasses the full cognitive cycle for <5s response times.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.fast_path]

        # Enable fast-path for trivial goals
        # When enabled, trivial goals (like "hello") skip the full cognitive cycle.
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__FAST_PATH__ENABLED
        enabled = true

        # Use cached responses for identical/similar queries
        # Default Value: true
        cache_enabled = true

        # Maximum cache entries
        # Default Value: 100
        cache_max_entries = 100

        # Cache TTL in seconds (1 hour default)
        # Default Value: 3600
        cache_ttl_seconds = 3600

        # Use template responses for known trivial patterns
        # Default Value: true
        templates_enabled = true

        # Maximum response time before timeout (milliseconds)
        # Default Value: 5000
        max_response_time_ms = 5000

        # Temperature for fast-path LLM calls
        # Default Value: 0.7
        temperature = 0.7

        # Maximum tokens for fast-path responses
        # Default Value: 500
        max_tokens = 500

        # Fall back to full cognitive cycle on timeout
        # Default Value: true
        fallback_on_timeout = true

        # ==============================================================================
        # [agents.circuit_breaker] - Circuit Breaker (G3 Phase 4)
        # ==============================================================================
        # Configures the circuit breaker to detect and interrupt failing agent loops.
        # Prevents infinite loops, runaway costs, and repeated errors.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.circuit_breaker]

        # Enable circuit breaker protection
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__CIRCUIT_BREAKER__ENABLED
        enabled = true

        # Maximum iterations (overrides agent max_iterations as hard limit)
        # Default Value: 15
        max_iterations = 15

        # Trip after N identical consecutive errors
        # Default Value: 3
        # Environment Variable: LLMCORE_AGENTS__CIRCUIT_BREAKER__MAX_SAME_ERRORS
        max_same_errors = 3

        # Trip after N seconds of total execution time
        # Default Value: 300 (5 minutes)
        max_execution_time_seconds = 300

        # Trip if total cost exceeds this amount (USD)
        # Default Value: 1.0
        max_total_cost = 1.0

        # Trip if progress stalls for N iterations (no output change)
        # Default Value: 5
        progress_stall_threshold = 5

        # Progress similarity tolerance (0.0 = exact match, 1.0 = any difference)
        # Default Value: 0.01
        progress_stall_tolerance = 0.01

        # ==============================================================================
        # [agents.activities] - Activity System (G3 Phase 6)
        # ==============================================================================
        # Configures the activity system for models without native tool/function calling.
        # Activities use XML-based structured output as an alternative to function calls.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.activities]

        # Enable activity system as fallback for models without native tool support
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__ACTIVITIES__ENABLED
        enabled = true

        # Fall back to native tools if activity parsing fails
        # Default Value: true
        fallback_to_native_tools = true

        # Maximum activities per LLM output iteration
        # Default Value: 10
        max_per_iteration = 10

        # Maximum total activities per session
        # Default Value: 100
        max_total = 100

        # Default timeout for activity execution (seconds)
        # Default Value: 60
        default_timeout_seconds = 60

        # Total timeout for activity loop (seconds)
        # Default Value: 300
        total_timeout_seconds = 300

        # Stop processing activities on first error
        # Default Value: false
        stop_on_error = false

        # Execute activities in parallel when possible
        # Default Value: false
        parallel_execution = false

        # Maximum observation length in characters
        # Default Value: 4000
        max_observation_length = 4000

        # ==============================================================================
        # [agents.capability_check] - Pre-flight Capability Checking (G3 Phase 5)
        # ==============================================================================
        # Configures pre-flight capability checking before agent execution.
        # Validates that the selected model supports required features (tools, etc.)
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.capability_check]

        # Enable pre-flight capability checking
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__CAPABILITY_CHECK__ENABLED
        enabled = true

        # Consult model card registry for capability information
        # Default Value: true
        use_model_cards = true

        # Use runtime query to verify capabilities (if model card not found)
        # Default Value: true
        use_runtime_query = true

        # Strict mode: fail if model doesn't support required capabilities
        # When false, falls back to activity system for non-tool models
        # Default Value: false
        # Environment Variable: LLMCORE_AGENTS__CAPABILITY_CHECK__STRICT_MODE
        strict_mode = false

        # Suggest alternative models if capability check fails
        # Default Value: true
        suggest_alternatives = true

        # ==============================================================================
        # [agents.hitl] - Human-in-the-Loop (G3 Phase 7)
        # ==============================================================================
        # Configures human-in-the-loop approval workflows for sensitive operations.
        # Allows human review of agent actions before execution.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.hitl]

        # Enable HITL approval system
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__HITL__ENABLED
        enabled = true

        # Global risk threshold for requiring approval
        # Possible Values: "low", "medium", "high", "critical"
        # Default Value: "medium"
        # Environment Variable: LLMCORE_AGENTS__HITL__GLOBAL_RISK_THRESHOLD
        global_risk_threshold = "medium"

        # Default timeout for approval requests (seconds)
        # Default Value: 300 (5 minutes)
        default_timeout_seconds = 300

        # Policy when approval times out
        # Possible Values: "reject", "approve", "escalate"
        # Default Value: "reject"
        timeout_policy = "reject"

        # Batch similar approval requests
        # Default Value: true
        batch_similar_requests = true

        # Enable audit logging of all approval decisions
        # Default Value: true
        audit_logging_enabled = true

        # Path for HITL audit log (JSONL format)
        # Default Value: "~/.llmcore/logs/hitl_audit.jsonl"
        audit_log_path = "~/.llmcore/logs/hitl_audit.jsonl"

        # ==============================================================================
        # [agents.routing] - Model Routing (G3 Phase 5)
        # ==============================================================================
        # Configures intelligent model routing based on task requirements.
        # Routes goals to optimal models based on complexity, cost, and capability.
        #
        # Reference: LLMCORE_AGENTIC_SYSTEM_MASTER_PLAN_G3.md Section 8

        [agents.routing]

        # Enable model routing
        # Default Value: true
        # Environment Variable: LLMCORE_AGENTS__ROUTING__ENABLED
        enabled = true

        # Routing optimization strategy
        # Possible Values: "cost_optimized", "quality_optimized", "latency_optimized"
        # Default Value: "cost_optimized"
        strategy = "cost_optimized"

        # Enable fallback to alternative models on failure
        # Default Value: true
        fallback_enabled = true

        # Model tiers for routing (fastest/cheapest to most capable)
        [agents.routing.tiers]
        fast = ["gpt-4o-mini", "claude-3-haiku-20240307", "gemma3:1b"]
        balanced = ["gpt-4o", "claude-3-5-sonnet-20241022", "llama3.3:70b"]
        capable = ["gpt-4-turbo", "claude-3-opus-20240229", "o1-preview"]
