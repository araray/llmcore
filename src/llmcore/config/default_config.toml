# ==============================================================================
# == LLMCore v0.21.0 - Comprehensive & Self-Documented Configuration File      ==
# ==============================================================================
#
# This file serves as the complete template for all settings available in LLMCore.
# It is heavily commented to explain the purpose, options, and default value
# for each configuration parameter.
#
# --- Configuration Override Precedence (Lowest to Highest) ---
# 1. Packaged Defaults: The settings defined in this file.
# 2. User Config File: Located at `~/.config/llmcore/config.toml`.
# 3. Custom Config File: A path passed during `LLMCore.create()`.
# 4. `.env` File: Variables loaded from a `.env` file in your project root.
# 5. Environment Variables: System-level variables (e.g., `export LLMCORE_...`).
# 6. Code Overrides: A dictionary passed directly to `LLMCore.create()`.
#
# For production, it is STRONGLY recommended to use environment variables for
# secrets like API keys. The corresponding environment variable for each setting
# is noted in its comment block.

# ==============================================================================
# [llmcore] - Global Settings
# ==============================================================================
# This section defines top-level settings that control the library's default
# behavior and global features.
[llmcore]

# --- Default LLM Provider ---
# What it does: Specifies the default LLM provider to use for any `chat()` call
#               where a provider is not explicitly named.
# Purpose: Simplifies API calls by setting your primary provider once.
# Possible Options: Any string that matches a configured provider instance name
#                   under the `[providers]` section (e.g., "openai", "ollama").
# Default Value: "ollama"
# Environment Variable: LLMCORE_DEFAULT_PROVIDER
default_provider = "ollama"

# --- Default Embedding Model for RAG ---
# What it does: Sets the default model used for all Retrieval Augmented
#               Generation (RAG) operations, like adding documents to the
#               vector store and performing similarity searches.
# Purpose: Centralizes the choice of embedding model for RAG, ensuring
#          consistency across your application's knowledge base.
# Possible Options:
#   - For local Sentence Transformers: The model name from Hugging Face Hub
#     (e.g., "all-MiniLM-L6-v2"). The model will be downloaded automatically.
#   - For service-based models: A string in the format "provider:model_name"
#     (e.g., "openai:text-embedding-3-small", "google:models/embedding-001").
# Default Value: "all-MiniLM-L6-v2"
# Environment Variable: LLMCORE_DEFAULT_EMBEDDING_MODEL
default_embedding_model = "all-MiniLM-L6-v2"

# --- Logging Verbosity ---
# What it does: Configures the logging verbosity for the entire `llmcore` library.
# Purpose: Control the amount of diagnostic information emitted by the library.
# Possible Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
# Default Value: "INFO"
# Environment Variable: LLMCORE_LOG_LEVEL
log_level = "INFO"

# --- Raw Payload Logging (for Debugging) ---
# What it does: If true, `llmcore` will log the exact raw JSON request and
#               response payloads exchanged with the underlying LLM provider APIs.
#               This logging occurs at the `DEBUG` log level.
# Purpose: An indispensable tool for advanced troubleshooting. It allows you to
#          inspect the precise data being sent and received, which is essential
#          for diagnosing provider errors or understanding unexpected behavior.
# Possible Options: true, false
# Default Value: false
# Environment Variable: LLMCORE_LOG_RAW_PAYLOADS
log_raw_payloads = false


# ==============================================================================
# [providers] - LLM Provider Configurations
# ==============================================================================
# This section is where all individual LLM provider instances are defined.
# You can define multiple instances of the same provider type by giving them
# unique names (e.g., [providers.my_openai_clone]) and setting `type = "openai"`.
[providers]

  # --- OpenAI Provider ---
  # For official OpenAI models and any OpenAI-compatible API (e.g., Groq, Anyscale).
  [providers.openai]
  # API Key: It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_PROVIDERS__OPENAI__API_KEY or OPENAI_API_KEY
  # api_key = "sk-..."

  # Base URL: Override for proxies or compatible APIs.
  # Example for Groq: base_url = "https://api.groq.com/openai/v1"
  # base_url = "https://api.openai.com/v1"

  # Default model for this provider instance.
  default_model = "gpt-4o"

  # Request timeout in seconds.
  timeout = 60

  # --- Anthropic Provider ---
  # For Claude models.
  [providers.anthropic]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__ANTHROPIC__API_KEY or ANTHROPIC_API_KEY
  # api_key = "sk-ant-..."

  # Default model for this provider instance.
  default_model = "claude-3-opus-20240229"
  timeout = 60

  # --- Ollama Provider ---
  # For locally running models via Ollama.
  [providers.ollama]
  # Host URL of the running Ollama server.
  # The official `ollama` Python library uses this parameter.
  # host = "http://localhost:11434"

  # Default local model to use. Ensure you have run `ollama pull <model_name>`.
  default_model = "llama3"

  # Longer timeout is recommended for local models.
  timeout = 120

  # Tokenizer for token counting. `tiktoken_cl100k_base` is a good default.
  # Use 'char_div_4' as a rough fallback if tiktoken causes issues with a model.
  # tokenizer = "tiktoken_cl100k_base"

  # --- Google Gemini Provider ---
  # For Gemini models.
  [providers.gemini]
  # API Key: Set via environment variable for security.
  # Environment Variable: LLMCORE_PROVIDERS__GEMINI__API_KEY or GOOGLE_API_KEY
  # api_key = "..."

  # Default model for this provider instance.
  default_model = "gemini-1.5-pro-latest"
  timeout = 60

  # Optional: Configure Google AI's safety filters.
  # This table is passed directly to the `google-genai` SDK.
  # [providers.gemini.safety_settings]
  # HARM_CATEGORY_HARASSMENT = "BLOCK_NONE"
  # HARM_CATEGORY_HATE_SPEECH = "BLOCK_NONE"


# ==============================================================================
# [storage] - Persistence Configurations
# ==============================================================================
# This section governs where and how persistent data, such as conversation
# history (session memory) and RAG documents (semantic memory), is stored.
[storage]

  # --- Session & Episodic Memory Storage ---
  # Configures the backend for storing conversation history and agent episodes.
  [storage.session]
  # Type: Determines the storage backend.
  #   - "json": Simple, human-readable. Good for low-concurrency apps.
  #   - "sqlite": (Default) Excellent for single-process apps and local dev.
  #   - "postgres": Recommended for production, especially with multiple workers.
  # Environment Variable: LLMCORE_STORAGE__SESSION__TYPE
  type = "sqlite"

  # Path: Filesystem path for "json" and "sqlite" backends.
  # The `~` character is expanded to your home directory.
  # Environment Variable: LLMCORE_STORAGE__SESSION__PATH
  path = "~/.llmcore/sessions.db"

  # Database URL: Full connection URL for "postgres".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__SESSION__DB_URL
  # Example: db_url = "postgresql://user:password@host:port/database_name"
  db_url = ""

  # --- Vector Storage for RAG (Semantic Memory) ---
  # Configures the vector store for RAG documents and their embeddings.
  [storage.vector]
  # Type: Determines the vector database backend.
  #   - "chromadb": (Default) Easy to set up for local development.
  #   - "pgvector": Excellent for production if you already use PostgreSQL.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__TYPE
  type = "chromadb"

  # Default Collection: A namespace for RAG documents. Allows you to maintain
  # multiple distinct knowledge bases in the same vector store.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DEFAULT_COLLECTION
  default_collection = "llmcore_default_rag"

  # Path: Filesystem path for file-based vector stores like "chromadb".
  # Environment Variable: LLMCORE_STORAGE__VECTOR__PATH
  path = "~/.llmcore/chroma_db"

  # Database URL: Connection URL for "pgvector".
  # It is STRONGLY recommended to set this via an environment variable.
  # Environment Variable: LLMCORE_STORAGE__VECTOR__DB_URL
  db_url = ""


# ==============================================================================
# [embedding] - Vectorization Model Configurations
# ==============================================================================
# This section configures the models responsible for converting text into
# numerical vectors (embeddings) for the RAG system. The global
# `llmcore.default_embedding_model` setting determines which model to use.
# If that identifier contains a colon (e.g., "openai:text-embedding-3-small"),
# the part before the colon tells the system which configuration block below
# to consult for specific settings like API keys.
[embedding]

  [embedding.openai]
  # API Key for OpenAI embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__OPENAI__API_KEY
  # api_key = ""
  default_model = "text-embedding-3-small"
  # base_url = "..." # Optional proxy URL

  [embedding.google]
  # API Key for Google AI (Gemini) embeddings.
  # Environment Variable: LLMCORE_EMBEDDING__GOOGLE__API_KEY
  # api_key = ""
  default_model = "models/embedding-001"

  [embedding.ollama]
  # Host URL for the Ollama server for embeddings.
  # host = "http://localhost:11434"
  # Default Ollama model for embeddings. Ensure it's pulled locally.
  # Examples: "mxbai-embed-large", "nomic-embed-text"
  default_model = "mxbai-embed-large"
  timeout = 60

  [embedding.sentence_transformer]
  # Additional configuration for local `sentence-transformers` models.
  # The model itself is specified in `llmcore.default_embedding_model`.
  # Device to run the model on. If not set, the library will auto-detect.
  # Options: "cpu", "cuda", "mps"
  # device = "cpu"


# ==============================================================================
# [context_management] - Advanced Prompt Control
# ==============================================================================
# This section is the control center for prompt engineering. It governs the
# sophisticated, multi-stage process of assembling the final context that is
# sent to the LLM. The system first prioritizes what to INCLUDE, and only then
# decides what to TRUNCATE if the model's token limit is exceeded.
[context_management]

  # --- Inclusion Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               components are added to the prompt.
  # Purpose: Gives fine-grained control over the final prompt's structure.
  # Valid Components: "system_history", "explicitly_staged", "user_items_active",
  #                   "history_chat", "final_user_query".
  # Default: "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"
  inclusion_priority = "system_history,explicitly_staged,user_items_active,history_chat,final_user_query"

  # --- Truncation Priority ---
  # What it does: A comma-separated list defining the order in which context
  #               types are removed to meet token limits.
  # Purpose: Controls what information is sacrificed when the context is too long.
  # Valid Components: "history_chat", "user_items_active", "rag_in_query",
  #                   "explicitly_staged".
  # Default: "history_chat,user_items_active,rag_in_query,explicitly_staged"
  truncation_priority = "history_chat,user_items_active,rag_in_query,explicitly_staged"

  # --- RAG Prompt Template ---
  # What it does: The template used to format the final query when RAG is enabled.
  #               It will be formatted with `{context}` (retrieved documents) and
  #               `{question}` (the user's query).
  # Purpose: To instruct the LLM on how to use the provided RAG context.
  default_prompt_template = """You are an AI assistant specialized in answering questions about codebases based on provided context.
Use ONLY the following pieces of retrieved context to answer the user's question.
If the answer is not found in the context, state that you cannot answer based on the provided information.
Do not make up an answer or use external knowledge. Keep the answer concise and relevant to the question.
Include relevant source file paths and line numbers if possible, based *only* on the provided context metadata.

Context:
---------------------
{context}
---------------------

Question: {question}

Answer:"""

  # Optional path to a custom prompt template file. Overrides the string above.
  prompt_template_path = ""

  # Default number of documents to retrieve for RAG.
  rag_retrieval_k = 3

  # Number of tokens to leave free in the context window for the model's response.
  reserved_response_tokens = 500

  # Minimum number of recent chat messages to try to keep during truncation.
  minimum_history_messages = 1

  # Prioritizes keeping the N most recent user messages (and their preceding
  # assistant responses) during truncation.
  user_retained_messages_count = 5

  # A safeguard character limit for a single user-provided context item.
  # The default is very large to accommodate large documents.
  max_chars_per_user_item = 40000000


# ==============================================================================
# [apykatu] - Integrated Data Ingestion Pipeline Configuration
# ==============================================================================
# This section configures the `apykatu` data ingestion library. These settings
# are passed to the `ingest_data_task` background job, which is executed by the
# TaskMaster service. This allows you to manage the entire data ingestion
# pipeline from this single configuration file.
[apykatu]

  [apykatu.database]
  # Vector store type `apykatu` will write to.
  type = "chromadb"

  [apykatu.embeddings]
  # Default embedding model for `apykatu` to use during ingestion.
  # This can be different from `llmcore.default_embedding_model`.
  # The name must match a key under `[apykatu.embeddings.models]`.
  default_model = "apykatu_default_st_embedder"

    [apykatu.embeddings.models]
      [apykatu.embeddings.models.apykatu_default_st_embedder]
      provider = "sentence-transformers"
      model_name = "all-MiniLM-L6-v2"
      device = "cpu"

  [apykatu.chunking]
  # Default strategy for breaking down documents.
  default_strategy = "RecursiveSplitter"

    [apykatu.chunking.strategies]
      # Example of a code-aware chunking strategy for Python files.
      [apykatu.chunking.strategies.python_code]
      extensions = [".py", ".pyw"]
      grammar = "Python3"
      entry_points =

    [apykatu.chunking.parameters]
      # Parameters for the RecursiveSplitter strategy.

      chunk_size = 800
      chunk_overlap = 100

  [apykatu.discovery]
  # If true, `apykatu` will respect rules in `.gitignore` files.
  use_gitignore = true
  # List of directory names to always exclude.
  excluded_dirs = ["__pycache__", "node_modules", "build", "dist", "venv", ".venv", "target", ".git"]
  # List of file patterns to always exclude.
  excluded_files =
